# S3 Storage Management

The S3 Storage module in Framework3 provides a powerful and scalable way to store and retrieve data using Amazon S3 (Simple Storage Service). This module is particularly useful for cloud-based storage, distributed computing, and managing large datasets in data processing pipelines.

## Overview

The `S3Storage` class implements the `BaseStorage` interface, offering a set of methods for file operations on Amazon S3. It provides functionality for uploading, downloading, listing, and deleting files, as well as checking for file existence in S3 buckets.

## Key Features

- File upload and download to/from S3
- S3 bucket management
- File existence checking in S3
- File listing from S3 buckets
- File deletion from S3
- Support for direct file streaming
- Integration with AWS credentials and regions

## Usage

### Basic Usage

To use the S3 Storage, first import and instantiate the `S3Storage` class:

```python
from framework3.plugins.storage.s3_storage import S3Storage

# Initialize S3Storage with AWS credentials and bucket information
storage = S3Storage(
    bucket='my-s3-bucket',
    region_name='us-west-2',
    access_key_id='YOUR_ACCESS_KEY_ID',
    access_key='YOUR_SECRET_ACCESS_KEY'
)
```

### Uploading Files

```python
content = "Hello, World!"
file_name = "greeting.txt"
context = "messages"

# Upload a file to S3
storage.upload_file(file=content, file_name=file_name, context=context)
```

### Downloading Files

```python
# Download a file from S3
downloaded_content = storage.download_file(file_name, context)
print(downloaded_content)  # Outputs: Hello, World!
```

### Checking File Existence

```python
# Check if a file exists in S3
exists = storage.check_if_exists(file_name, context)
print(exists)  # Outputs: True
```

### Listing Files

```python
# List files in a context (S3 prefix)
files = storage.list_stored_files(context)
print(files)  # Outputs: ['greeting.txt']
```

### Deleting Files

```python
# Delete a file from S3
storage.delete_file(file_name, context)
```

## Advanced Usage

### Direct File Streaming

For large files or when memory efficiency is crucial, you can use direct file streaming:

```python
import io

large_data = io.BytesIO(b"Large amount of data...")
storage.upload_file(file=large_data, file_name='large_file.dat', context='data', direct_stream=True)
```

### Custom S3 Endpoint

If you're using a custom S3-compatible storage service, you can specify a custom endpoint:

```python
storage = S3Storage(
    bucket='my-bucket',
    region_name='us-east-1',
    access_key_id='YOUR_ACCESS_KEY_ID',
    access_key='YOUR_SECRET_ACCESS_KEY',
    endpoint_url='https://custom-s3-endpoint.com'
)
```

## Best Practices

1. **Security**: Always use IAM roles or temporary credentials when possible, instead of hardcoding AWS access keys.
2. **Error Handling**: Implement proper error handling to manage S3 operation failures gracefully.
3. **Context Usage**: Organize your files using contexts (S3 prefixes) for better file management.
4. **Lifecycle Policies**: Set up S3 lifecycle policies to manage storage costs and data retention.
5. **Versioning**: Consider enabling S3 versioning for critical data to protect against accidental deletions or overwrites.
6. **Encryption**: Use server-side encryption for sensitive data stored in S3.

## API Reference

For a detailed look at the `S3Storage` class and its methods, refer to the auto-generated documentation below:

::: framework3.plugins.storage.s3_storage.S3Storage

## Conclusion

The S3 Storage module in Framework3 provides a robust and scalable solution for managing file operations on Amazon S3. By leveraging this module, you can easily integrate cloud storage capabilities into your data processing pipelines, enabling distributed computing and efficient management of large datasets.
