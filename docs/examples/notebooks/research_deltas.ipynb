{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1bfef9f",
   "metadata": {},
   "source": [
    "# Temporal Word Embeddings for Early Detection of Psychological Disorders on Social Media"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5538f",
   "metadata": {},
   "source": [
    "## How to early detect psychological disorders on social media using temporal word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b372eb",
   "metadata": {},
   "source": [
    "#### Abastract\n",
    "*Mental health disorders represent a public health challenge, where early detection is critical to mitigating adverse outcomes for individuals and society. The study of language and behavior is a pivotal component in mental health research, and the content from social media platforms serves as a valuable tool for identifying signs of mental health risks. This paper presents a novel framework leveraging temporal word embeddings to capture linguistic changes over time. We specifically aim at at identifying emerging psychological concerns on social media. By adapting temporal word representations, our approach quantifies shifts in language use that may signal mental health risks. To that end, we implement two alternative temporal word embedding models to detect linguistic variations and exploit these variations to train early detection classifiers. Our experiments, conducted on 18 datasets from the eRisk initiative (covering signs of conditions such as depression, anorexia, and self-harm), show that simple models focusing exclusively on temporal word usage patterns achieve competitive performance compared to state-of-the-art systems. Additionally, we perform a word-level analysis to understand the evolution of key terms among positive and control users. These findings underscore the potential of time-sensitive word models in this domain, being a promising avenue for future research in mental health surveillance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad778d5",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c40a2a",
   "metadata": {},
   "source": [
    "#### **TWEC**\n",
    "\n",
    "In this tutorial, we will focus exclusively on TWEC. Let's begin by defining our temporal word embedding models. The first model, TWEC (Temporal Word Embeddings with a Compass), is an extension of Word2Vec that incorporates temporal information. It captures linguistic shifts over time by leveraging the context of surrounding words across different time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d7c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.twec import TWEC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b87c6b",
   "metadata": {},
   "source": [
    "### Deltas\n",
    "\n",
    "Deltas is a metric designed to quantify semantic drift in word meaning over time within a diachronic corpus. It is computed by applying similarity measures—such as cosine similarity or Euclidean distance—between temporally contextualized word embeddings and their corresponding static representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bebd90de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.deltas import DISTANCES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eac6cd9",
   "metadata": {},
   "source": [
    "## Filters\n",
    "\n",
    "Now that we have defined our models, we need to encapsulate them inside a class that implements the `BaseFilter` interface and binds them to the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a48958a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: framework3==1.0.15 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (1.0.15)\n",
      "Requirement already satisfied: boto3==1.35.73 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3==1.0.15) (1.35.73)\n",
      "Requirement already satisfied: cloudpickle==3.1.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3==1.0.15) (3.1.0)\n",
      "Requirement already satisfied: dill==0.3.9 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3==1.0.15) (0.3.9)\n",
      "Requirement already satisfied: fastapi==0.115.5 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3==1.0.15) (0.115.5)\n",
      "Requirement already satisfied: gensim==4.3.3 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3==1.0.15) (4.3.3)\n",
      "Requirement already satisfied: multimethod==1.12 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3==1.0.15) (1.12)\n",
      "Requirement already satisfied: nltk==3.9.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3==1.0.15) (3.9.1)\n",
      "Requirement already satisfied: optuna==4.2.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3==1.0.15) (4.2.1)\n",
      "Requirement already satisfied: pandas==2.2.3 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3==1.0.15) (2.2.3)\n",
      "Requirement already satisfied: pyspark==3.5.3 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3==1.0.15) (3.5.3)\n",
      "Requirement already satisfied: rich==13.9.4 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3==1.0.15) (13.9.4)\n",
      "Requirement already satisfied: scikit-learn==1.5.2 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3==1.0.15) (1.5.2)\n",
      "Requirement already satisfied: scipy==1.13.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3==1.0.15) (1.13.1)\n",
      "Requirement already satisfied: sentence-transformers==4.0.2 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3==1.0.15) (4.0.2)\n",
      "Requirement already satisfied: torch==2.5.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3==1.0.15) (2.5.1)\n",
      "Requirement already satisfied: tqdm==4.67.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3==1.0.15) (4.67.1)\n",
      "Requirement already satisfied: transformers==4.51.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3==1.0.15) (4.51.1)\n",
      "Requirement already satisfied: typeguard==4.4.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3==1.0.15) (4.4.1)\n",
      "Requirement already satisfied: wandb==0.19.9 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3==1.0.15) (0.19.9)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.73 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from boto3==1.35.73->framework3==1.0.15) (1.35.74)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from boto3==1.35.73->framework3==1.0.15) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from boto3==1.35.73->framework3==1.0.15) (0.10.4)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from fastapi==0.115.5->framework3==1.0.15) (0.41.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from fastapi==0.115.5->framework3==1.0.15) (2.10.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from fastapi==0.115.5->framework3==1.0.15) (4.12.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from gensim==4.3.3->framework3==1.0.15) (1.26.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from gensim==4.3.3->framework3==1.0.15) (7.0.5)\n",
      "Requirement already satisfied: click in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from nltk==3.9.1->framework3==1.0.15) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from nltk==3.9.1->framework3==1.0.15) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from nltk==3.9.1->framework3==1.0.15) (2024.11.6)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from optuna==4.2.1->framework3==1.0.15) (1.15.2)\n",
      "Requirement already satisfied: colorlog in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from optuna==4.2.1->framework3==1.0.15) (6.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from optuna==4.2.1->framework3==1.0.15) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from optuna==4.2.1->framework3==1.0.15) (2.0.40)\n",
      "Requirement already satisfied: PyYAML in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from optuna==4.2.1->framework3==1.0.15) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from pandas==2.2.3->framework3==1.0.15) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from pandas==2.2.3->framework3==1.0.15) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from pandas==2.2.3->framework3==1.0.15) (2024.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from pyspark==3.5.3->framework3==1.0.15) (0.10.9.7)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from rich==13.9.4->framework3==1.0.15) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from rich==13.9.4->framework3==1.0.15) (2.18.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from scikit-learn==1.5.2->framework3==1.0.15) (3.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from sentence-transformers==4.0.2->framework3==1.0.15) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from sentence-transformers==4.0.2->framework3==1.0.15) (11.1.0)\n",
      "Requirement already satisfied: filelock in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3==1.0.15) (3.16.1)\n",
      "Requirement already satisfied: networkx in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3==1.0.15) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3==1.0.15) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3==1.0.15) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3==1.0.15) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3==1.0.15) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3==1.0.15) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3==1.0.15) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3==1.0.15) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3==1.0.15) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3==1.0.15) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3==1.0.15) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3==1.0.15) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3==1.0.15) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3==1.0.15) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3==1.0.15) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3==1.0.15) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3==1.0.15) (1.13.1)\n",
      "Requirement already satisfied: requests in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from transformers==4.51.1->framework3==1.0.15) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from transformers==4.51.1->framework3==1.0.15) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from transformers==4.51.1->framework3==1.0.15) (0.4.5)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from wandb==0.19.9->framework3==1.0.15) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from wandb==0.19.9->framework3==1.0.15) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from wandb==0.19.9->framework3==1.0.15) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from wandb==0.19.9->framework3==1.0.15) (5.29.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from wandb==0.19.9->framework3==1.0.15) (6.1.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from wandb==0.19.9->framework3==1.0.15) (2.19.0)\n",
      "Requirement already satisfied: setproctitle in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from wandb==0.19.9->framework3==1.0.15) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from wandb==0.19.9->framework3==1.0.15) (75.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.5.1->framework3==1.0.15) (1.3.0)\n",
      "Requirement already satisfied: Mako in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from alembic>=1.5.0->optuna==4.2.1->framework3==1.0.15) (1.3.10)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from botocore<1.36.0,>=1.35.73->boto3==1.35.73->framework3==1.0.15) (2.2.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb==0.19.9->framework3==1.0.15) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb==0.19.9->framework3==1.0.15) (4.0.11)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich==13.9.4->framework3==1.0.15) (0.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.115.5->framework3==1.0.15) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.115.5->framework3==1.0.15) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from requests->transformers==4.51.1->framework3==1.0.15) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from requests->transformers==4.51.1->framework3==1.0.15) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from requests->transformers==4.51.1->framework3==1.0.15) (2024.8.30)\n",
      "Requirement already satisfied: wrapt in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim==4.3.3->framework3==1.0.15) (1.17.0)\n",
      "Requirement already satisfied: greenlet>=1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from sqlalchemy>=1.4.2->optuna==4.2.1->framework3==1.0.15) (3.1.1)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from starlette<0.42.0,>=0.40.0->fastapi==0.115.5->framework3==1.0.15) (4.6.2.post1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from jinja2->torch==2.5.1->framework3==1.0.15) (3.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi==0.115.5->framework3==1.0.15) (1.3.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.19.9->framework3==1.0.15) (5.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install framework3==1.0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a85993e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Patched inspect.getsource using dill.\n"
     ]
    }
   ],
   "source": [
    "from framework3.utils.patch_type_guard import patch_inspect_for_notebooks\n",
    "\n",
    "patch_inspect_for_notebooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d7819c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Literal\n",
    "from scipy.sparse import dok_matrix\n",
    "from tqdm import tqdm\n",
    "from framework3.base.base_clases import BaseFilter\n",
    "from framework3.base.base_types import XYData\n",
    "from framework3.container import Container\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "@Container.bind()\n",
    "class TWECFilter(BaseFilter):\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_size: int,\n",
    "        _cpus: int = 4,\n",
    "        deltas_f: List[\n",
    "            Literal[\n",
    "                \"cosine\",\n",
    "                \"euclidean\",\n",
    "                \"chebyshev\",\n",
    "                \"jensen_shannon\",\n",
    "                \"wasserstein\",\n",
    "                \"manhattan\",\n",
    "                \"minkowski\",\n",
    "            ]\n",
    "        ] = [\"cosine\"],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._twec = TWEC(size=300, window=context_size)\n",
    "        self.deltas_f = deltas_f\n",
    "        self.context_size = context_size\n",
    "        actual_cpus = os.cpu_count()\n",
    "        if actual_cpus is not None:\n",
    "            self._cpus = min(actual_cpus, _cpus)\n",
    "        else:\n",
    "            self._cpus = _cpus\n",
    "\n",
    "    def fit(self, x: XYData, y: XYData | None) -> float | None:\n",
    "        data: pd.DataFrame = x.value\n",
    "        self._twec.train_compass(data.text.values.tolist())\n",
    "        self._vocab_hash_map = dict(\n",
    "            zip(\n",
    "                self._twec.compass.wv.index_to_key,  # type: ignore\n",
    "                range(len(self._twec.compass.wv.index_to_key)),  # type: ignore\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def predict(self, x: XYData) -> XYData:\n",
    "        data: pd.DataFrame = x.value\n",
    "        n_rows = len(data.index)\n",
    "        n_cols = len(self._vocab_hash_map.items())\n",
    "        metric_names = self.deltas_f\n",
    "\n",
    "        all_deltas = {\n",
    "            metric: dok_matrix((n_rows, n_cols), dtype=np.float32)\n",
    "            for metric in metric_names\n",
    "        }\n",
    "\n",
    "        def process_user_deltas(i, tc):\n",
    "            result = {metric: [] for metric in metric_names}\n",
    "            for word in tc.wv.index_to_key:  # type: ignore\n",
    "                if word in self._vocab_hash_map:\n",
    "                    j = self._vocab_hash_map[word]\n",
    "                    for metric in metric_names:\n",
    "                        dist = (\n",
    "                            DISTANCES[metric](\n",
    "                                torch.tensor(np.array([[self._twec.compass.wv[word]]])),  # type: ignore\n",
    "                                torch.tensor(np.array([[tc.wv[word]]])),  # type: ignore\n",
    "                            )\n",
    "                            .detach()\n",
    "                            .cpu()\n",
    "                            .item()\n",
    "                        )\n",
    "                        result[metric].append((i, j, dist))\n",
    "            return result\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self._cpus) as executor:\n",
    "            futures = {\n",
    "                executor.submit(\n",
    "                    process_user_deltas, i, self._twec.train_slice(row.text)\n",
    "                ): i\n",
    "                for i, row in tqdm(\n",
    "                    enumerate(data.itertuples()),\n",
    "                    total=n_rows,\n",
    "                    desc=\"generating embeddings\",\n",
    "                )\n",
    "            }\n",
    "\n",
    "            for future in tqdm(\n",
    "                as_completed(futures), total=n_rows, desc=\"parallel prediction\"\n",
    "            ):\n",
    "                chunk_result = future.result()\n",
    "                for metric, values in chunk_result.items():\n",
    "                    for i, j, val in values:\n",
    "                        all_deltas[metric][i, j] = val\n",
    "\n",
    "        return XYData.mock(all_deltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676eada9",
   "metadata": {},
   "source": [
    "## The classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8730068",
   "metadata": {},
   "source": [
    "This work addresses an early prediction task using the eRisk dataset, which requires the use of classification models. We will now define a set of classifiers and integrate them within the framework by wrapping them in the appropriate classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8acfe51",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "⚠️ **Warning:** In order for classes to be parallelizable, they must be defined in a standalone module. For this reason, we have moved the classifiers to separate files. The code shown here is provided for reference purposes only.\n",
    "\n",
    "⚠️ **Warning:** Also note that some hyperparameters are not primitive types. While this works well with `sklearn` and `Optuna` optimizers, it may break when using the `wandb` optimizer. The code should be adapted accordingly if you plan to use `wandb` for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20336758",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954205d4",
   "metadata": {},
   "source": [
    "```python\n",
    "from typing import Callable, Mapping\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "@Container.bind()\n",
    "class ClassifierSVM(BaseFilter):\n",
    "    def __init__(\n",
    "        self,\n",
    "        C: float = 1,\n",
    "        kernel: Callable | Literal['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'] = \"rbf\",\n",
    "        gamma: float | Literal['scale', 'auto'] = \"scale\",\n",
    "        coef0:float=0.0,\n",
    "        tol:float=0.001,\n",
    "        decision_function_shape:Literal['ovo', 'ovr'] = \"ovr\",\n",
    "        class_weight_1: Mapping[Any, Any] | str | None = None,\n",
    "        probability:bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.proba = probability\n",
    "        self._model = SVC(\n",
    "            C=C,\n",
    "            kernel=kernel,\n",
    "            gamma=gamma,\n",
    "            coef0=coef0,\n",
    "            tol=tol,\n",
    "            decision_function_shape=decision_function_shape,\n",
    "            class_weight={1: class_weight_1},\n",
    "            probability=probability,\n",
    "            random_state=43,\n",
    "        )\n",
    "\n",
    "    def fit(self, x: XYData, y: XYData | None):\n",
    "        if y is None:\n",
    "            raise ValueError(\"y must be provided for training\")\n",
    "        self._model.fit(x.value, y.value)\n",
    "\n",
    "    def predict(self, x: XYData) -> XYData:\n",
    "        if self.proba:\n",
    "            result = list(map(lambda i: i[1], self._model.predict_proba(x.value)))\n",
    "            return XYData.mock(result)\n",
    "        else:\n",
    "            result = self._model.predict(x.value)\n",
    "            return XYData.mock(result)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cd1967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.svm import ClassifierSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341ce8a3",
   "metadata": {},
   "source": [
    "## The Metrics\n",
    "\n",
    "Since the early prediction task is essentially a classification problem, we will use standard classification metrics such as **F1-score**, **Precision**, and **Recall**. However, due to the *early* nature of the task, we also need to include metrics that penalize delayed decisions, as timing is a critical aspect of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8c7a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from framework3.plugins.metrics import F1, Precission, Recall\n",
    "\n",
    "f1 = F1()\n",
    "precision = Precission()\n",
    "recall = Recall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0f2a12",
   "metadata": {},
   "source": [
    "```python\n",
    "from typing import Iterable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from framework3 import BaseMetric\n",
    "\n",
    "from numpy import exp\n",
    "\n",
    "@Container.bind()\n",
    "class ERDE(BaseMetric):\n",
    "    def __init__(self, count: Iterable, k: int = 5):\n",
    "        self.k = k\n",
    "        self.count = count\n",
    "\n",
    "    def evaluate(\n",
    "        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n",
    "    ) -> float | np.ndarray:\n",
    "        if y_true is None:\n",
    "            raise ValueError(\"y_true must be provided for evaluation\")\n",
    "\n",
    "        all_erde = []\n",
    "        _, _, _, tp = confusion_matrix(y_true.value, y_pred.value).ravel()\n",
    "        for expected, result, count in list(\n",
    "            zip(y_true.value, y_pred.value, self.count)\n",
    "        ):\n",
    "            if result == 1 and expected == 0:\n",
    "                all_erde.append(float(tp) / len(y_true.value))\n",
    "            elif result == 0 and expected == 1:\n",
    "                all_erde.append(1.0)\n",
    "            elif result == 1 and expected == 1:\n",
    "                all_erde.append(1.0 - (1.0 / (1.0 + exp(count - self.k))))\n",
    "            elif result == 0 and expected == 0:\n",
    "                all_erde.append(0.0)\n",
    "        return float(np.mean(all_erde) * 100)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08acfb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics.erde import ERDE_5, ERDE_50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4cfa59",
   "metadata": {},
   "source": [
    "#### For simplicity, we will only consider the 2023 gambling data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "248cd043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>chunk</th>\n",
       "      <th>label</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject5539_0</td>\n",
       "      <td>For PC: I don't know what company, but they ne...</td>\n",
       "      <td>2015-12-03 13:31:29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>subject5539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject5539_1</td>\n",
       "      <td>You play as a Pokmon trainer (that you customi...</td>\n",
       "      <td>2015-12-04 19:45:40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>subject5539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject5539_2</td>\n",
       "      <td>A Clash of Clans RPG (or MMORPG)</td>\n",
       "      <td>2015-12-23 23:32:51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>subject5539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject5539_3</td>\n",
       "      <td>You would have to manage your species's needs ...</td>\n",
       "      <td>2015-12-26 20:45:30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>subject5539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject5539_4</td>\n",
       "      <td>The game starts you as a child and you have to...</td>\n",
       "      <td>2016-01-02 08:40:18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>subject5539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                               text  \\\n",
       "0  subject5539_0  For PC: I don't know what company, but they ne...   \n",
       "1  subject5539_1  You play as a Pokmon trainer (that you customi...   \n",
       "2  subject5539_2                   A Clash of Clans RPG (or MMORPG)   \n",
       "3  subject5539_3  You would have to manage your species's needs ...   \n",
       "4  subject5539_4  The game starts you as a child and you have to...   \n",
       "\n",
       "                  date  chunk  label         user  \n",
       "0  2015-12-03 13:31:29      0      0  subject5539  \n",
       "1  2015-12-04 19:45:40      0      0  subject5539  \n",
       "2  2015-12-23 23:32:51      0      0  subject5539  \n",
       "3  2015-12-26 20:45:30      0      0  subject5539  \n",
       "4  2016-01-02 08:40:18      0      0  subject5539  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gambling_2023_train = pd.read_csv(\"data/standard_gambling_train_2023.csv\", index_col=0)\n",
    "gambling_2023_train.head(5)\n",
    "\n",
    "gambling_2023_test = pd.read_csv(\"data/standard_gambling_2023.csv\", index_col=0)\n",
    "gambling_2023_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1278690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>chunk</th>\n",
       "      <th>n_texts</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject1</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>[Vulcan's ultimate landing at max range is so ...</td>\n",
       "      <td>[2017-08-18 11:34:09, 2017-08-20 15:26:34, 201...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject1</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>[Awesome! It is always good to hear these news...</td>\n",
       "      <td>[2018-05-18 23:46:33, 2018-06-18 17:17:55, 201...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject1</td>\n",
       "      <td>2</td>\n",
       "      <td>132</td>\n",
       "      <td>[The syringe is a lie!, I'd say Scylla or Than...</td>\n",
       "      <td>[2018-09-20 08:20:44, 2018-09-24 10:12:03, 201...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject1</td>\n",
       "      <td>3</td>\n",
       "      <td>131</td>\n",
       "      <td>[Some of the symptoms you may experience are b...</td>\n",
       "      <td>[2019-05-06 17:50:52, 2019-05-06 19:05:44, 201...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject1</td>\n",
       "      <td>4</td>\n",
       "      <td>132</td>\n",
       "      <td>[So ur saying that huge map is better than Afg...</td>\n",
       "      <td>[2019-10-06 23:22:46, 2019-10-12 18:08:06, 201...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39265</th>\n",
       "      <td>subject9999</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[10v10, is those a bunch of bots, I didn't eve...</td>\n",
       "      <td>[2021-07-04 06:51:04, 2021-07-04 07:01:05, 202...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39266</th>\n",
       "      <td>subject9999</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>[I'm commenting this based on the fact that Am...</td>\n",
       "      <td>[2021-07-15 19:11:42, 2021-07-27 17:44:09, 202...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39267</th>\n",
       "      <td>subject9999</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>[Aesthetic set, It's a fucking downgrade,, It'...</td>\n",
       "      <td>[2021-09-12 14:55:05, 2021-09-23 00:31:11, 202...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39268</th>\n",
       "      <td>subject9999</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[u/save, u/savevideo, Snu snu ! Snu snu! Snu s...</td>\n",
       "      <td>[2021-09-23 13:38:48, 2021-10-08 13:44:30, 202...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39269</th>\n",
       "      <td>subject9999</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>[Why every fucking time there's a new weapon o...</td>\n",
       "      <td>[2021-11-27 06:45:16, 2021-11-27 07:04:36, 202...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39270 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              user  chunk  n_texts  \\\n",
       "0         subject1      0      132   \n",
       "1         subject1      1      132   \n",
       "2         subject1      2      132   \n",
       "3         subject1      3      131   \n",
       "4         subject1      4      132   \n",
       "...            ...    ...      ...   \n",
       "39265  subject9999      5        8   \n",
       "39266  subject9999      6        8   \n",
       "39267  subject9999      7        8   \n",
       "39268  subject9999      8        8   \n",
       "39269  subject9999      9        8   \n",
       "\n",
       "                                                    text  \\\n",
       "0      [Vulcan's ultimate landing at max range is so ...   \n",
       "1      [Awesome! It is always good to hear these news...   \n",
       "2      [The syringe is a lie!, I'd say Scylla or Than...   \n",
       "3      [Some of the symptoms you may experience are b...   \n",
       "4      [So ur saying that huge map is better than Afg...   \n",
       "...                                                  ...   \n",
       "39265  [10v10, is those a bunch of bots, I didn't eve...   \n",
       "39266  [I'm commenting this based on the fact that Am...   \n",
       "39267  [Aesthetic set, It's a fucking downgrade,, It'...   \n",
       "39268  [u/save, u/savevideo, Snu snu ! Snu snu! Snu s...   \n",
       "39269  [Why every fucking time there's a new weapon o...   \n",
       "\n",
       "                                                    date  label  \n",
       "0      [2017-08-18 11:34:09, 2017-08-20 15:26:34, 201...      0  \n",
       "1      [2018-05-18 23:46:33, 2018-06-18 17:17:55, 201...      0  \n",
       "2      [2018-09-20 08:20:44, 2018-09-24 10:12:03, 201...      0  \n",
       "3      [2019-05-06 17:50:52, 2019-05-06 19:05:44, 201...      0  \n",
       "4      [2019-10-06 23:22:46, 2019-10-12 18:08:06, 201...      0  \n",
       "...                                                  ...    ...  \n",
       "39265  [2021-07-04 06:51:04, 2021-07-04 07:01:05, 202...      0  \n",
       "39266  [2021-07-15 19:11:42, 2021-07-27 17:44:09, 202...      0  \n",
       "39267  [2021-09-12 14:55:05, 2021-09-23 00:31:11, 202...      0  \n",
       "39268  [2021-09-23 13:38:48, 2021-10-08 13:44:30, 202...      0  \n",
       "39269  [2021-11-27 06:45:16, 2021-11-27 07:04:36, 202...      0  \n",
       "\n",
       "[39270 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg_2023_train = (\n",
    "    gambling_2023_train.groupby([\"user\", \"chunk\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"id\": \"count\",\n",
    "            \"text\": list,\n",
    "            \"date\": list,\n",
    "            \"label\": \"first\",\n",
    "        }\n",
    "    )\n",
    "    .rename(columns={\"id\": \"n_texts\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "gg_2023_test = (\n",
    "    gambling_2023_test.groupby([\"user\", \"chunk\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"id\": \"count\",\n",
    "            \"text\": list,\n",
    "            \"date\": list,\n",
    "            \"label\": \"first\",\n",
    "        }\n",
    "    )\n",
    "    .rename(columns={\"id\": \"n_texts\"})\n",
    "    .reset_index()\n",
    ")\n",
    "gg_2023_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae64ae02",
   "metadata": {},
   "source": [
    "⚠️ **Warning:** There are several restrictions for the plugins to work properly:\n",
    "\n",
    "- Constructor arguments should be public attributes.\n",
    "- Other data must be set as private attributes.\n",
    "- All public attributes must be serializable using `jsonable_encoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48369484",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_erde_5 = ERDE_5(gg_2023_test.n_texts.values.tolist())\n",
    "test_erde_50 = ERDE_50(gg_2023_test.n_texts.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edc8ecd",
   "metadata": {},
   "source": [
    "#### Selector\n",
    "\n",
    "We are using `sklearn` for grid search. This optimizer will check the input dimensions of the `X` and `y` values. We have generated a dictionary with the deltas based on different distance measures, but this results in an incompatible dimensions error from `sklearn`. To work around this issue, we define a class that selects the appropriate deltas based on a hyperparameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8952fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Container.bind()\n",
    "class DeltaSelectorFilter(BaseFilter):\n",
    "    def __init__(\n",
    "        self,\n",
    "        deltas_f: Literal[\n",
    "            \"cosine\",\n",
    "            \"euclidean\",\n",
    "            \"chebyshev\",\n",
    "            \"jensen_shannon\",\n",
    "            \"wasserstein\",\n",
    "            \"manhattan\",\n",
    "            \"minkowski\",\n",
    "        ] = \"cosine\",\n",
    "    ):\n",
    "        self.deltas_f = deltas_f\n",
    "\n",
    "    def fit(self, x: XYData, y: XYData | None):\n",
    "        pass\n",
    "\n",
    "    def predict(self, x: XYData) -> XYData:\n",
    "        # Crear una nueva dok_matrix con el mismo shape y en float32\n",
    "        old_dok = x.value[self.deltas_f]\n",
    "        new_dok = dok_matrix(old_dok.shape, dtype=np.float32)\n",
    "\n",
    "        # Copiar todos los valores existentes y convertir el dtype\n",
    "        for (i, j), value in old_dok.items():\n",
    "            new_dok[i, j] = float(value)  # conversión a float32 implícita\n",
    "        return XYData.mock(new_dok.tocsr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f3c130",
   "metadata": {},
   "source": [
    "#### The pipeline\n",
    "\n",
    "Now comes the most exciting part: integrating the filters into the pipeline. This step can be done incrementally, which is more convenient when developing a model. However, since we already have a clear understanding of the process, we will combine all the parts into one step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d973f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages/framework3/base/base_clases.py:134: InstrumentationWarning: instrumentor did not find the target function -- not typechecking __main__.TWECFilter.__init__\n",
      "  cls.__init__ = typechecked(init_method)  # type: ignore[method-assign]\n",
      "/home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages/framework3/base/base_clases.py:142: InstrumentationWarning: instrumentor did not find the target function -- not typechecking __main__.TWECFilter.fit\n",
      "  setattr(cls, attr_name, typechecked(attr_value))\n",
      "/home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages/framework3/base/base_clases.py:142: InstrumentationWarning: instrumentor did not find the target function -- not typechecking __main__.TWECFilter.predict\n",
      "  setattr(cls, attr_name, typechecked(attr_value))\n",
      "/home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages/framework3/base/base_clases.py:134: InstrumentationWarning: instrumentor did not find the target function -- not typechecking __main__.DeltaSelectorFilter.__init__\n",
      "  cls.__init__ = typechecked(init_method)  # type: ignore[method-assign]\n",
      "/home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages/framework3/base/base_clases.py:142: InstrumentationWarning: instrumentor did not find the target function -- not typechecking __main__.DeltaSelectorFilter.fit\n",
      "  setattr(cls, attr_name, typechecked(attr_value))\n",
      "/home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages/framework3/base/base_clases.py:142: InstrumentationWarning: instrumentor did not find the target function -- not typechecking __main__.DeltaSelectorFilter.predict\n",
      "  setattr(cls, attr_name, typechecked(attr_value))\n"
     ]
    }
   ],
   "source": [
    "from framework3 import Cached, SklearnOptimizer\n",
    "from framework3.plugins.pipelines.sequential import F3Pipeline\n",
    "\n",
    "all_test_metrics = [\n",
    "    f1,\n",
    "    precision,\n",
    "    recall,\n",
    "    test_erde_5,\n",
    "    test_erde_50,\n",
    "]\n",
    "\n",
    "pipeline_svm = F3Pipeline(\n",
    "    filters=[\n",
    "        Cached(\n",
    "            filter=TWECFilter(\n",
    "                context_size=25,\n",
    "                _cpus=10,\n",
    "                deltas_f=[\"cosine\", \"euclidean\", \"manhattan\", \"chebyshev\"],\n",
    "            ),\n",
    "        ),\n",
    "        DeltaSelectorFilter(deltas_f=\"cosine\"),\n",
    "        F3Pipeline(\n",
    "            filters=[\n",
    "                ClassifierSVM(\n",
    "                    tol=0.003,\n",
    "                    probability=False,\n",
    "                    decision_function_shape=\"ovr\",\n",
    "                    kernel=\"rbf\",\n",
    "                    gamma=\"scale\",\n",
    "                ).grid(\n",
    "                    {\n",
    "                        \"C\": [1, 3, 5],\n",
    "                        \"class_weight_1\": [{1: 1.5}, {1: 2.5}, {1: 3.0}],\n",
    "                    }\n",
    "                )\n",
    "            ],\n",
    "            metrics=[F1()],\n",
    "        ).optimizer(SklearnOptimizer(scoring=\"f1_weighted\", cv=2, n_jobs=-1)),\n",
    "    ],\n",
    "    metrics=all_test_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dc952b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clazz': 'F3Pipeline',\n",
       " 'params': {'filters': [{'clazz': 'Cached',\n",
       "    'params': {'filter': {'clazz': 'TWECFilter',\n",
       "      'params': {'deltas_f': ['cosine', 'euclidean', 'manhattan', 'chebyshev'],\n",
       "       'context_size': 25}},\n",
       "     'cache_data': True,\n",
       "     'cache_filter': True,\n",
       "     'overwrite': False,\n",
       "     'storage': None}},\n",
       "   {'clazz': 'DeltaSelectorFilter', 'params': {'deltas_f': 'cosine'}},\n",
       "   {'clazz': 'SklearnOptimizer',\n",
       "    'params': {'scoring': 'f1_weighted',\n",
       "     'cv': 2,\n",
       "     'pipeline': {'clazz': 'F3Pipeline',\n",
       "      'params': {'filters': [{'clazz': 'ClassifierSVM',\n",
       "         'params': {'proba': False}}],\n",
       "       'metrics': [{'clazz': 'F1', 'params': {'average': 'weighted'}}],\n",
       "       'overwrite': False,\n",
       "       'store': False,\n",
       "       'log': False}},\n",
       "     'n_jobs': -1}}],\n",
       "  'metrics': [{'clazz': 'F1', 'params': {'average': 'weighted'}},\n",
       "   {'clazz': 'Precission', 'params': {'average': 'weighted'}},\n",
       "   {'clazz': 'Recall', 'params': {'average': 'weighted'}},\n",
       "   {'clazz': 'ERDE_5',\n",
       "    'params': {'erde': {'clazz': 'ERDE',\n",
       "      'params': {'count': [64,\n",
       "        64,\n",
       "        64,\n",
       "        64,\n",
       "        64,\n",
       "        64,\n",
       "        64,\n",
       "        64,\n",
       "        64,\n",
       "        64,\n",
       "        134,\n",
       "        133,\n",
       "        133,\n",
       "        133,\n",
       "        133,\n",
       "        134,\n",
       "        133,\n",
       "        133,\n",
       "        133,\n",
       "        133,\n",
       "        10,\n",
       "        10,\n",
       "        9,\n",
       "        10,\n",
       "        9,\n",
       "        10,\n",
       "        10,\n",
       "        9,\n",
       "        10,\n",
       "        9,\n",
       "        51,\n",
       "        50,\n",
       "        51,\n",
       "        50,\n",
       "        51,\n",
       "        50,\n",
       "        51,\n",
       "        50,\n",
       "        51,\n",
       "        50,\n",
       "        27,\n",
       "        27,\n",
       "        27,\n",
       "        27,\n",
       "        26,\n",
       "        27,\n",
       "        27,\n",
       "        27,\n",
       "        27,\n",
       "        26,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        199,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        199,\n",
       "        108,\n",
       "        107,\n",
       "        107,\n",
       "        107,\n",
       "        107,\n",
       "        108,\n",
       "        107,\n",
       "        107,\n",
       "        107,\n",
       "        107,\n",
       "        3,\n",
       "        3,\n",
       "        3,\n",
       "        3,\n",
       "        2,\n",
       "        3,\n",
       "        3,\n",
       "        3,\n",
       "        3,\n",
       "        2,\n",
       "        59,\n",
       "        58,\n",
       "        58,\n",
       "        58,\n",
       "        58,\n",
       "        59,\n",
       "        58,\n",
       "        58,\n",
       "        58,\n",
       "        58,\n",
       "        126,\n",
       "        125,\n",
       "        125,\n",
       "        125,\n",
       "        125,\n",
       "        126,\n",
       "        125,\n",
       "        125,\n",
       "        125,\n",
       "        125,\n",
       "        11,\n",
       "        11,\n",
       "        11,\n",
       "        10,\n",
       "        11,\n",
       "        11,\n",
       "        10,\n",
       "        11,\n",
       "        11,\n",
       "        10,\n",
       "        17,\n",
       "        16,\n",
       "        16,\n",
       "        16,\n",
       "        16,\n",
       "        16,\n",
       "        16,\n",
       "        16,\n",
       "        16,\n",
       "        16,\n",
       "        2,\n",
       "        1,\n",
       "        2,\n",
       "        1,\n",
       "        1,\n",
       "        2,\n",
       "        1,\n",
       "        2,\n",
       "        1,\n",
       "        1,\n",
       "        41,\n",
       "        40,\n",
       "        40,\n",
       "        40,\n",
       "        40,\n",
       "        41,\n",
       "        40,\n",
       "        40,\n",
       "        40,\n",
       "        40,\n",
       "        10,\n",
       "        10,\n",
       "        9,\n",
       "        10,\n",
       "        9,\n",
       "        10,\n",
       "        10,\n",
       "        9,\n",
       "        10,\n",
       "        9,\n",
       "        199,\n",
       "        198,\n",
       "        199,\n",
       "        198,\n",
       "        198,\n",
       "        199,\n",
       "        198,\n",
       "        199,\n",
       "        198,\n",
       "        198,\n",
       "        103,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        103,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        46,\n",
       "        46,\n",
       "        46,\n",
       "        46,\n",
       "        45,\n",
       "        46,\n",
       "        46,\n",
       "        46,\n",
       "        46,\n",
       "        45,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        25,\n",
       "        23,\n",
       "        23,\n",
       "        22,\n",
       "        23,\n",
       "        22,\n",
       "        23,\n",
       "        23,\n",
       "        22,\n",
       "        23,\n",
       "        22,\n",
       "        23,\n",
       "        22,\n",
       "        23,\n",
       "        22,\n",
       "        23,\n",
       "        22,\n",
       "        23,\n",
       "        22,\n",
       "        23,\n",
       "        22,\n",
       "        9,\n",
       "        8,\n",
       "        9,\n",
       "        8,\n",
       "        9,\n",
       "        8,\n",
       "        9,\n",
       "        8,\n",
       "        9,\n",
       "        8,\n",
       "        14,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        14,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        7,\n",
       "        7,\n",
       "        7,\n",
       "        7,\n",
       "        6,\n",
       "        7,\n",
       "        7,\n",
       "        7,\n",
       "        7,\n",
       "        6,\n",
       "        32,\n",
       "        32,\n",
       "        32,\n",
       "        31,\n",
       "        32,\n",
       "        32,\n",
       "        31,\n",
       "        32,\n",
       "        32,\n",
       "        31,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        64,\n",
       "        63,\n",
       "        63,\n",
       "        64,\n",
       "        63,\n",
       "        63,\n",
       "        64,\n",
       "        63,\n",
       "        63,\n",
       "        63,\n",
       "        42,\n",
       "        42,\n",
       "        42,\n",
       "        42,\n",
       "        41,\n",
       "        42,\n",
       "        42,\n",
       "        42,\n",
       "        42,\n",
       "        41,\n",
       "        17,\n",
       "        16,\n",
       "        17,\n",
       "        16,\n",
       "        16,\n",
       "        17,\n",
       "        16,\n",
       "        17,\n",
       "        16,\n",
       "        16,\n",
       "        186,\n",
       "        185,\n",
       "        185,\n",
       "        186,\n",
       "        185,\n",
       "        185,\n",
       "        186,\n",
       "        185,\n",
       "        185,\n",
       "        185,\n",
       "        16,\n",
       "        15,\n",
       "        15,\n",
       "        15,\n",
       "        15,\n",
       "        16,\n",
       "        15,\n",
       "        15,\n",
       "        15,\n",
       "        15,\n",
       "        101,\n",
       "        100,\n",
       "        101,\n",
       "        100,\n",
       "        101,\n",
       "        100,\n",
       "        101,\n",
       "        100,\n",
       "        101,\n",
       "        100,\n",
       "        2,\n",
       "        1,\n",
       "        1,\n",
       "        2,\n",
       "        1,\n",
       "        1,\n",
       "        2,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        32,\n",
       "        31,\n",
       "        31,\n",
       "        32,\n",
       "        31,\n",
       "        31,\n",
       "        32,\n",
       "        31,\n",
       "        31,\n",
       "        31,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        5,\n",
       "        174,\n",
       "        173,\n",
       "        173,\n",
       "        173,\n",
       "        173,\n",
       "        173,\n",
       "        173,\n",
       "        173,\n",
       "        173,\n",
       "        173,\n",
       "        38,\n",
       "        37,\n",
       "        37,\n",
       "        37,\n",
       "        37,\n",
       "        37,\n",
       "        37,\n",
       "        37,\n",
       "        37,\n",
       "        37,\n",
       "        119,\n",
       "        118,\n",
       "        119,\n",
       "        118,\n",
       "        119,\n",
       "        118,\n",
       "        119,\n",
       "        118,\n",
       "        119,\n",
       "        118,\n",
       "        6,\n",
       "        5,\n",
       "        5,\n",
       "        6,\n",
       "        5,\n",
       "        5,\n",
       "        6,\n",
       "        5,\n",
       "        5,\n",
       "        5,\n",
       "        179,\n",
       "        178,\n",
       "        178,\n",
       "        179,\n",
       "        178,\n",
       "        178,\n",
       "        179,\n",
       "        178,\n",
       "        178,\n",
       "        178,\n",
       "        114,\n",
       "        114,\n",
       "        114,\n",
       "        114,\n",
       "        114,\n",
       "        114,\n",
       "        114,\n",
       "        114,\n",
       "        114,\n",
       "        113,\n",
       "        5,\n",
       "        5,\n",
       "        5,\n",
       "        4,\n",
       "        5,\n",
       "        5,\n",
       "        4,\n",
       "        5,\n",
       "        5,\n",
       "        4,\n",
       "        5,\n",
       "        5,\n",
       "        5,\n",
       "        5,\n",
       "        4,\n",
       "        5,\n",
       "        5,\n",
       "        5,\n",
       "        5,\n",
       "        4,\n",
       "        48,\n",
       "        47,\n",
       "        48,\n",
       "        47,\n",
       "        48,\n",
       "        47,\n",
       "        48,\n",
       "        47,\n",
       "        48,\n",
       "        47,\n",
       "        188,\n",
       "        188,\n",
       "        187,\n",
       "        188,\n",
       "        187,\n",
       "        188,\n",
       "        188,\n",
       "        187,\n",
       "        188,\n",
       "        187,\n",
       "        26,\n",
       "        25,\n",
       "        25,\n",
       "        25,\n",
       "        25,\n",
       "        25,\n",
       "        25,\n",
       "        25,\n",
       "        25,\n",
       "        25,\n",
       "        5,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        71,\n",
       "        71,\n",
       "        71,\n",
       "        71,\n",
       "        70,\n",
       "        71,\n",
       "        71,\n",
       "        71,\n",
       "        71,\n",
       "        70,\n",
       "        2,\n",
       "        2,\n",
       "        2,\n",
       "        2,\n",
       "        1,\n",
       "        2,\n",
       "        2,\n",
       "        2,\n",
       "        2,\n",
       "        1,\n",
       "        138,\n",
       "        138,\n",
       "        137,\n",
       "        138,\n",
       "        137,\n",
       "        138,\n",
       "        138,\n",
       "        137,\n",
       "        138,\n",
       "        137,\n",
       "        80,\n",
       "        80,\n",
       "        80,\n",
       "        80,\n",
       "        80,\n",
       "        80,\n",
       "        80,\n",
       "        80,\n",
       "        80,\n",
       "        79,\n",
       "        88,\n",
       "        88,\n",
       "        88,\n",
       "        88,\n",
       "        87,\n",
       "        88,\n",
       "        88,\n",
       "        88,\n",
       "        88,\n",
       "        87,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        14,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        35,\n",
       "        35,\n",
       "        35,\n",
       "        35,\n",
       "        35,\n",
       "        35,\n",
       "        35,\n",
       "        35,\n",
       "        35,\n",
       "        35,\n",
       "        7,\n",
       "        6,\n",
       "        6,\n",
       "        7,\n",
       "        6,\n",
       "        6,\n",
       "        7,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        99,\n",
       "        99,\n",
       "        99,\n",
       "        99,\n",
       "        99,\n",
       "        99,\n",
       "        99,\n",
       "        99,\n",
       "        99,\n",
       "        99,\n",
       "        24,\n",
       "        24,\n",
       "        24,\n",
       "        24,\n",
       "        24,\n",
       "        24,\n",
       "        24,\n",
       "        24,\n",
       "        24,\n",
       "        24,\n",
       "        5,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        115,\n",
       "        114,\n",
       "        114,\n",
       "        115,\n",
       "        114,\n",
       "        114,\n",
       "        115,\n",
       "        114,\n",
       "        114,\n",
       "        114,\n",
       "        26,\n",
       "        26,\n",
       "        25,\n",
       "        26,\n",
       "        25,\n",
       "        26,\n",
       "        26,\n",
       "        25,\n",
       "        26,\n",
       "        25,\n",
       "        9,\n",
       "        9,\n",
       "        9,\n",
       "        9,\n",
       "        9,\n",
       "        9,\n",
       "        9,\n",
       "        9,\n",
       "        9,\n",
       "        8,\n",
       "        59,\n",
       "        59,\n",
       "        59,\n",
       "        59,\n",
       "        59,\n",
       "        59,\n",
       "        59,\n",
       "        59,\n",
       "        59,\n",
       "        58,\n",
       "        55,\n",
       "        54,\n",
       "        54,\n",
       "        54,\n",
       "        54,\n",
       "        54,\n",
       "        54,\n",
       "        54,\n",
       "        54,\n",
       "        54,\n",
       "        12,\n",
       "        11,\n",
       "        12,\n",
       "        11,\n",
       "        11,\n",
       "        12,\n",
       "        11,\n",
       "        12,\n",
       "        11,\n",
       "        11,\n",
       "        98,\n",
       "        97,\n",
       "        97,\n",
       "        97,\n",
       "        97,\n",
       "        97,\n",
       "        97,\n",
       "        97,\n",
       "        97,\n",
       "        97,\n",
       "        118,\n",
       "        117,\n",
       "        117,\n",
       "        117,\n",
       "        117,\n",
       "        118,\n",
       "        117,\n",
       "        117,\n",
       "        117,\n",
       "        117,\n",
       "        8,\n",
       "        7,\n",
       "        8,\n",
       "        7,\n",
       "        8,\n",
       "        7,\n",
       "        8,\n",
       "        7,\n",
       "        8,\n",
       "        7,\n",
       "        166,\n",
       "        166,\n",
       "        166,\n",
       "        166,\n",
       "        166,\n",
       "        166,\n",
       "        166,\n",
       "        166,\n",
       "        166,\n",
       "        166,\n",
       "        198,\n",
       "        198,\n",
       "        197,\n",
       "        198,\n",
       "        197,\n",
       "        198,\n",
       "        198,\n",
       "        197,\n",
       "        198,\n",
       "        197,\n",
       "        198,\n",
       "        197,\n",
       "        198,\n",
       "        197,\n",
       "        197,\n",
       "        198,\n",
       "        197,\n",
       "        198,\n",
       "        197,\n",
       "        197,\n",
       "        18,\n",
       "        17,\n",
       "        17,\n",
       "        17,\n",
       "        17,\n",
       "        17,\n",
       "        17,\n",
       "        17,\n",
       "        17,\n",
       "        17,\n",
       "        28,\n",
       "        27,\n",
       "        27,\n",
       "        27,\n",
       "        27,\n",
       "        28,\n",
       "        27,\n",
       "        27,\n",
       "        27,\n",
       "        27,\n",
       "        101,\n",
       "        101,\n",
       "        100,\n",
       "        101,\n",
       "        100,\n",
       "        101,\n",
       "        101,\n",
       "        100,\n",
       "        101,\n",
       "        100,\n",
       "        10,\n",
       "        10,\n",
       "        10,\n",
       "        10,\n",
       "        10,\n",
       "        10,\n",
       "        10,\n",
       "        10,\n",
       "        10,\n",
       "        9,\n",
       "        2,\n",
       "        2,\n",
       "        2,\n",
       "        1,\n",
       "        2,\n",
       "        2,\n",
       "        1,\n",
       "        2,\n",
       "        2,\n",
       "        1,\n",
       "        48,\n",
       "        47,\n",
       "        48,\n",
       "        47,\n",
       "        47,\n",
       "        48,\n",
       "        47,\n",
       "        48,\n",
       "        47,\n",
       "        47,\n",
       "        47,\n",
       "        47,\n",
       "        47,\n",
       "        47,\n",
       "        47,\n",
       "        47,\n",
       "        47,\n",
       "        47,\n",
       "        47,\n",
       "        46,\n",
       "        61,\n",
       "        61,\n",
       "        61,\n",
       "        61,\n",
       "        61,\n",
       "        61,\n",
       "        61,\n",
       "        61,\n",
       "        61,\n",
       "        60,\n",
       "        12,\n",
       "        12,\n",
       "        12,\n",
       "        12,\n",
       "        12,\n",
       "        12,\n",
       "        12,\n",
       "        12,\n",
       "        12,\n",
       "        12,\n",
       "        78,\n",
       "        78,\n",
       "        78,\n",
       "        78,\n",
       "        77,\n",
       "        78,\n",
       "        78,\n",
       "        78,\n",
       "        78,\n",
       "        77,\n",
       "        23,\n",
       "        22,\n",
       "        22,\n",
       "        23,\n",
       "        22,\n",
       "        22,\n",
       "        23,\n",
       "        22,\n",
       "        22,\n",
       "        22,\n",
       "        31,\n",
       "        30,\n",
       "        31,\n",
       "        30,\n",
       "        31,\n",
       "        30,\n",
       "        31,\n",
       "        30,\n",
       "        31,\n",
       "        30,\n",
       "        2,\n",
       "        2,\n",
       "        2,\n",
       "        1,\n",
       "        2,\n",
       "        2,\n",
       "        1,\n",
       "        2,\n",
       "        2,\n",
       "        1,\n",
       "        39,\n",
       "        38,\n",
       "        38,\n",
       "        38,\n",
       "        38,\n",
       "        39,\n",
       "        38,\n",
       "        38,\n",
       "        38,\n",
       "        38,\n",
       "        4,\n",
       "        3,\n",
       "        3,\n",
       "        4,\n",
       "        3,\n",
       "        3,\n",
       "        4,\n",
       "        3,\n",
       "        3,\n",
       "        3,\n",
       "        104,\n",
       "        104,\n",
       "        104,\n",
       "        104,\n",
       "        104,\n",
       "        104,\n",
       "        104,\n",
       "        104,\n",
       "        104,\n",
       "        103,\n",
       "        105,\n",
       "        104,\n",
       "        105,\n",
       "        104,\n",
       "        104,\n",
       "        105,\n",
       "        104,\n",
       "        105,\n",
       "        104,\n",
       "        104,\n",
       "        5,\n",
       "        4,\n",
       "        5,\n",
       "        4,\n",
       "        5,\n",
       "        4,\n",
       "        5,\n",
       "        4,\n",
       "        5,\n",
       "        4,\n",
       "        9,\n",
       "        9,\n",
       "        9,\n",
       "        8,\n",
       "        9,\n",
       "        9,\n",
       "        8,\n",
       "        9,\n",
       "        9,\n",
       "        8,\n",
       "        41,\n",
       "        40,\n",
       "        40,\n",
       "        41,\n",
       "        40,\n",
       "        40,\n",
       "        41,\n",
       "        40,\n",
       "        40,\n",
       "        40,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        27,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        27,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        103,\n",
       "        103,\n",
       "        103,\n",
       "        103,\n",
       "        103,\n",
       "        103,\n",
       "        103,\n",
       "        103,\n",
       "        103,\n",
       "        102,\n",
       "        32,\n",
       "        32,\n",
       "        32,\n",
       "        32,\n",
       "        32,\n",
       "        32,\n",
       "        32,\n",
       "        32,\n",
       "        32,\n",
       "        31,\n",
       "        39,\n",
       "        38,\n",
       "        39,\n",
       "        38,\n",
       "        39,\n",
       "        38,\n",
       "        39,\n",
       "        38,\n",
       "        39,\n",
       "        38,\n",
       "        117,\n",
       "        116,\n",
       "        116,\n",
       "        116,\n",
       "        116,\n",
       "        117,\n",
       "        116,\n",
       "        116,\n",
       "        116,\n",
       "        116,\n",
       "        ...],\n",
       "       'k': 5}}}},\n",
       "   {'clazz': 'ERDE_50',\n",
       "    'params': {'erde': {'clazz': 'ERDE',\n",
       "      'params': {'count': [64,\n",
       "        64,\n",
       "        64,\n",
       "        64,\n",
       "        64,\n",
       "        64,\n",
       "        64,\n",
       "        64,\n",
       "        64,\n",
       "        64,\n",
       "        134,\n",
       "        133,\n",
       "        133,\n",
       "        133,\n",
       "        133,\n",
       "        134,\n",
       "        133,\n",
       "        133,\n",
       "        133,\n",
       "        133,\n",
       "        10,\n",
       "        10,\n",
       "        9,\n",
       "        10,\n",
       "        9,\n",
       "        10,\n",
       "        10,\n",
       "        9,\n",
       "        10,\n",
       "        9,\n",
       "        51,\n",
       "        50,\n",
       "        51,\n",
       "        50,\n",
       "        51,\n",
       "        50,\n",
       "        51,\n",
       "        50,\n",
       "        51,\n",
       "        50,\n",
       "        27,\n",
       "        27,\n",
       "        27,\n",
       "        27,\n",
       "        26,\n",
       "        27,\n",
       "        27,\n",
       "        27,\n",
       "        27,\n",
       "        26,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        199,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        199,\n",
       "        108,\n",
       "        107,\n",
       "        107,\n",
       "        107,\n",
       "        107,\n",
       "        108,\n",
       "        107,\n",
       "        107,\n",
       "        107,\n",
       "        107,\n",
       "        3,\n",
       "        3,\n",
       "        3,\n",
       "        3,\n",
       "        2,\n",
       "        3,\n",
       "        3,\n",
       "        3,\n",
       "        3,\n",
       "        2,\n",
       "        59,\n",
       "        58,\n",
       "        58,\n",
       "        58,\n",
       "        58,\n",
       "        59,\n",
       "        58,\n",
       "        58,\n",
       "        58,\n",
       "        58,\n",
       "        126,\n",
       "        125,\n",
       "        125,\n",
       "        125,\n",
       "        125,\n",
       "        126,\n",
       "        125,\n",
       "        125,\n",
       "        125,\n",
       "        125,\n",
       "        11,\n",
       "        11,\n",
       "        11,\n",
       "        10,\n",
       "        11,\n",
       "        11,\n",
       "        10,\n",
       "        11,\n",
       "        11,\n",
       "        10,\n",
       "        17,\n",
       "        16,\n",
       "        16,\n",
       "        16,\n",
       "        16,\n",
       "        16,\n",
       "        16,\n",
       "        16,\n",
       "        16,\n",
       "        16,\n",
       "        2,\n",
       "        1,\n",
       "        2,\n",
       "        1,\n",
       "        1,\n",
       "        2,\n",
       "        1,\n",
       "        2,\n",
       "        1,\n",
       "        1,\n",
       "        41,\n",
       "        40,\n",
       "        40,\n",
       "        40,\n",
       "        40,\n",
       "        41,\n",
       "        40,\n",
       "        40,\n",
       "        40,\n",
       "        40,\n",
       "        10,\n",
       "        10,\n",
       "        9,\n",
       "        10,\n",
       "        9,\n",
       "        10,\n",
       "        10,\n",
       "        9,\n",
       "        10,\n",
       "        9,\n",
       "        199,\n",
       "        198,\n",
       "        199,\n",
       "        198,\n",
       "        198,\n",
       "        199,\n",
       "        198,\n",
       "        199,\n",
       "        198,\n",
       "        198,\n",
       "        103,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        103,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        46,\n",
       "        46,\n",
       "        46,\n",
       "        46,\n",
       "        45,\n",
       "        46,\n",
       "        46,\n",
       "        46,\n",
       "        46,\n",
       "        45,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        25,\n",
       "        23,\n",
       "        23,\n",
       "        22,\n",
       "        23,\n",
       "        22,\n",
       "        23,\n",
       "        23,\n",
       "        22,\n",
       "        23,\n",
       "        22,\n",
       "        23,\n",
       "        22,\n",
       "        23,\n",
       "        22,\n",
       "        23,\n",
       "        22,\n",
       "        23,\n",
       "        22,\n",
       "        23,\n",
       "        22,\n",
       "        9,\n",
       "        8,\n",
       "        9,\n",
       "        8,\n",
       "        9,\n",
       "        8,\n",
       "        9,\n",
       "        8,\n",
       "        9,\n",
       "        8,\n",
       "        14,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        14,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        7,\n",
       "        7,\n",
       "        7,\n",
       "        7,\n",
       "        6,\n",
       "        7,\n",
       "        7,\n",
       "        7,\n",
       "        7,\n",
       "        6,\n",
       "        32,\n",
       "        32,\n",
       "        32,\n",
       "        31,\n",
       "        32,\n",
       "        32,\n",
       "        31,\n",
       "        32,\n",
       "        32,\n",
       "        31,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        64,\n",
       "        63,\n",
       "        63,\n",
       "        64,\n",
       "        63,\n",
       "        63,\n",
       "        64,\n",
       "        63,\n",
       "        63,\n",
       "        63,\n",
       "        42,\n",
       "        42,\n",
       "        42,\n",
       "        42,\n",
       "        41,\n",
       "        42,\n",
       "        42,\n",
       "        42,\n",
       "        42,\n",
       "        41,\n",
       "        17,\n",
       "        16,\n",
       "        17,\n",
       "        16,\n",
       "        16,\n",
       "        17,\n",
       "        16,\n",
       "        17,\n",
       "        16,\n",
       "        16,\n",
       "        186,\n",
       "        185,\n",
       "        185,\n",
       "        186,\n",
       "        185,\n",
       "        185,\n",
       "        186,\n",
       "        185,\n",
       "        185,\n",
       "        185,\n",
       "        16,\n",
       "        15,\n",
       "        15,\n",
       "        15,\n",
       "        15,\n",
       "        16,\n",
       "        15,\n",
       "        15,\n",
       "        15,\n",
       "        15,\n",
       "        101,\n",
       "        100,\n",
       "        101,\n",
       "        100,\n",
       "        101,\n",
       "        100,\n",
       "        101,\n",
       "        100,\n",
       "        101,\n",
       "        100,\n",
       "        2,\n",
       "        1,\n",
       "        1,\n",
       "        2,\n",
       "        1,\n",
       "        1,\n",
       "        2,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        102,\n",
       "        32,\n",
       "        31,\n",
       "        31,\n",
       "        32,\n",
       "        31,\n",
       "        31,\n",
       "        32,\n",
       "        31,\n",
       "        31,\n",
       "        31,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        5,\n",
       "        174,\n",
       "        173,\n",
       "        173,\n",
       "        173,\n",
       "        173,\n",
       "        173,\n",
       "        173,\n",
       "        173,\n",
       "        173,\n",
       "        173,\n",
       "        38,\n",
       "        37,\n",
       "        37,\n",
       "        37,\n",
       "        37,\n",
       "        37,\n",
       "        37,\n",
       "        37,\n",
       "        37,\n",
       "        37,\n",
       "        119,\n",
       "        118,\n",
       "        119,\n",
       "        118,\n",
       "        119,\n",
       "        118,\n",
       "        119,\n",
       "        118,\n",
       "        119,\n",
       "        118,\n",
       "        6,\n",
       "        5,\n",
       "        5,\n",
       "        6,\n",
       "        5,\n",
       "        5,\n",
       "        6,\n",
       "        5,\n",
       "        5,\n",
       "        5,\n",
       "        179,\n",
       "        178,\n",
       "        178,\n",
       "        179,\n",
       "        178,\n",
       "        178,\n",
       "        179,\n",
       "        178,\n",
       "        178,\n",
       "        178,\n",
       "        114,\n",
       "        114,\n",
       "        114,\n",
       "        114,\n",
       "        114,\n",
       "        114,\n",
       "        114,\n",
       "        114,\n",
       "        114,\n",
       "        113,\n",
       "        5,\n",
       "        5,\n",
       "        5,\n",
       "        4,\n",
       "        5,\n",
       "        5,\n",
       "        4,\n",
       "        5,\n",
       "        5,\n",
       "        4,\n",
       "        5,\n",
       "        5,\n",
       "        5,\n",
       "        5,\n",
       "        4,\n",
       "        5,\n",
       "        5,\n",
       "        5,\n",
       "        5,\n",
       "        4,\n",
       "        48,\n",
       "        47,\n",
       "        48,\n",
       "        47,\n",
       "        48,\n",
       "        47,\n",
       "        48,\n",
       "        47,\n",
       "        48,\n",
       "        47,\n",
       "        188,\n",
       "        188,\n",
       "        187,\n",
       "        188,\n",
       "        187,\n",
       "        188,\n",
       "        188,\n",
       "        187,\n",
       "        188,\n",
       "        187,\n",
       "        26,\n",
       "        25,\n",
       "        25,\n",
       "        25,\n",
       "        25,\n",
       "        25,\n",
       "        25,\n",
       "        25,\n",
       "        25,\n",
       "        25,\n",
       "        5,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        71,\n",
       "        71,\n",
       "        71,\n",
       "        71,\n",
       "        70,\n",
       "        71,\n",
       "        71,\n",
       "        71,\n",
       "        71,\n",
       "        70,\n",
       "        2,\n",
       "        2,\n",
       "        2,\n",
       "        2,\n",
       "        1,\n",
       "        2,\n",
       "        2,\n",
       "        2,\n",
       "        2,\n",
       "        1,\n",
       "        138,\n",
       "        138,\n",
       "        137,\n",
       "        138,\n",
       "        137,\n",
       "        138,\n",
       "        138,\n",
       "        137,\n",
       "        138,\n",
       "        137,\n",
       "        80,\n",
       "        80,\n",
       "        80,\n",
       "        80,\n",
       "        80,\n",
       "        80,\n",
       "        80,\n",
       "        80,\n",
       "        80,\n",
       "        79,\n",
       "        88,\n",
       "        88,\n",
       "        88,\n",
       "        88,\n",
       "        87,\n",
       "        88,\n",
       "        88,\n",
       "        88,\n",
       "        88,\n",
       "        87,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        200,\n",
       "        14,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        13,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        35,\n",
       "        35,\n",
       "        35,\n",
       "        35,\n",
       "        35,\n",
       "        35,\n",
       "        35,\n",
       "        35,\n",
       "        35,\n",
       "        35,\n",
       "        7,\n",
       "        6,\n",
       "        6,\n",
       "        7,\n",
       "        6,\n",
       "        6,\n",
       "        7,\n",
       "        6,\n",
       "        6,\n",
       "        6,\n",
       "        99,\n",
       "        99,\n",
       "        99,\n",
       "        99,\n",
       "        99,\n",
       "        99,\n",
       "        99,\n",
       "        99,\n",
       "        99,\n",
       "        99,\n",
       "        24,\n",
       "        24,\n",
       "        24,\n",
       "        24,\n",
       "        24,\n",
       "        24,\n",
       "        24,\n",
       "        24,\n",
       "        24,\n",
       "        24,\n",
       "        5,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        4,\n",
       "        115,\n",
       "        114,\n",
       "        114,\n",
       "        115,\n",
       "        114,\n",
       "        114,\n",
       "        115,\n",
       "        114,\n",
       "        114,\n",
       "        114,\n",
       "        26,\n",
       "        26,\n",
       "        25,\n",
       "        26,\n",
       "        25,\n",
       "        26,\n",
       "        26,\n",
       "        25,\n",
       "        26,\n",
       "        25,\n",
       "        9,\n",
       "        9,\n",
       "        9,\n",
       "        9,\n",
       "        9,\n",
       "        9,\n",
       "        9,\n",
       "        9,\n",
       "        9,\n",
       "        8,\n",
       "        59,\n",
       "        59,\n",
       "        59,\n",
       "        59,\n",
       "        59,\n",
       "        59,\n",
       "        59,\n",
       "        59,\n",
       "        59,\n",
       "        58,\n",
       "        55,\n",
       "        54,\n",
       "        54,\n",
       "        54,\n",
       "        54,\n",
       "        54,\n",
       "        54,\n",
       "        54,\n",
       "        54,\n",
       "        54,\n",
       "        12,\n",
       "        11,\n",
       "        12,\n",
       "        11,\n",
       "        11,\n",
       "        12,\n",
       "        11,\n",
       "        12,\n",
       "        11,\n",
       "        11,\n",
       "        98,\n",
       "        97,\n",
       "        97,\n",
       "        97,\n",
       "        97,\n",
       "        97,\n",
       "        97,\n",
       "        97,\n",
       "        97,\n",
       "        97,\n",
       "        118,\n",
       "        117,\n",
       "        117,\n",
       "        117,\n",
       "        117,\n",
       "        118,\n",
       "        117,\n",
       "        117,\n",
       "        117,\n",
       "        117,\n",
       "        8,\n",
       "        7,\n",
       "        8,\n",
       "        7,\n",
       "        8,\n",
       "        7,\n",
       "        8,\n",
       "        7,\n",
       "        8,\n",
       "        7,\n",
       "        166,\n",
       "        166,\n",
       "        166,\n",
       "        166,\n",
       "        166,\n",
       "        166,\n",
       "        166,\n",
       "        166,\n",
       "        166,\n",
       "        166,\n",
       "        198,\n",
       "        198,\n",
       "        197,\n",
       "        198,\n",
       "        197,\n",
       "        198,\n",
       "        198,\n",
       "        197,\n",
       "        198,\n",
       "        197,\n",
       "        198,\n",
       "        197,\n",
       "        198,\n",
       "        197,\n",
       "        197,\n",
       "        198,\n",
       "        197,\n",
       "        198,\n",
       "        197,\n",
       "        197,\n",
       "        18,\n",
       "        17,\n",
       "        17,\n",
       "        17,\n",
       "        17,\n",
       "        17,\n",
       "        17,\n",
       "        17,\n",
       "        17,\n",
       "        17,\n",
       "        28,\n",
       "        27,\n",
       "        27,\n",
       "        27,\n",
       "        27,\n",
       "        28,\n",
       "        27,\n",
       "        27,\n",
       "        27,\n",
       "        27,\n",
       "        101,\n",
       "        101,\n",
       "        100,\n",
       "        101,\n",
       "        100,\n",
       "        101,\n",
       "        101,\n",
       "        100,\n",
       "        101,\n",
       "        100,\n",
       "        10,\n",
       "        10,\n",
       "        10,\n",
       "        10,\n",
       "        10,\n",
       "        10,\n",
       "        10,\n",
       "        10,\n",
       "        10,\n",
       "        9,\n",
       "        2,\n",
       "        2,\n",
       "        2,\n",
       "        1,\n",
       "        2,\n",
       "        2,\n",
       "        1,\n",
       "        2,\n",
       "        2,\n",
       "        1,\n",
       "        48,\n",
       "        47,\n",
       "        48,\n",
       "        47,\n",
       "        47,\n",
       "        48,\n",
       "        47,\n",
       "        48,\n",
       "        47,\n",
       "        47,\n",
       "        47,\n",
       "        47,\n",
       "        47,\n",
       "        47,\n",
       "        47,\n",
       "        47,\n",
       "        47,\n",
       "        47,\n",
       "        47,\n",
       "        46,\n",
       "        61,\n",
       "        61,\n",
       "        61,\n",
       "        61,\n",
       "        61,\n",
       "        61,\n",
       "        61,\n",
       "        61,\n",
       "        61,\n",
       "        60,\n",
       "        12,\n",
       "        12,\n",
       "        12,\n",
       "        12,\n",
       "        12,\n",
       "        12,\n",
       "        12,\n",
       "        12,\n",
       "        12,\n",
       "        12,\n",
       "        78,\n",
       "        78,\n",
       "        78,\n",
       "        78,\n",
       "        77,\n",
       "        78,\n",
       "        78,\n",
       "        78,\n",
       "        78,\n",
       "        77,\n",
       "        23,\n",
       "        22,\n",
       "        22,\n",
       "        23,\n",
       "        22,\n",
       "        22,\n",
       "        23,\n",
       "        22,\n",
       "        22,\n",
       "        22,\n",
       "        31,\n",
       "        30,\n",
       "        31,\n",
       "        30,\n",
       "        31,\n",
       "        30,\n",
       "        31,\n",
       "        30,\n",
       "        31,\n",
       "        30,\n",
       "        2,\n",
       "        2,\n",
       "        2,\n",
       "        1,\n",
       "        2,\n",
       "        2,\n",
       "        1,\n",
       "        2,\n",
       "        2,\n",
       "        1,\n",
       "        39,\n",
       "        38,\n",
       "        38,\n",
       "        38,\n",
       "        38,\n",
       "        39,\n",
       "        38,\n",
       "        38,\n",
       "        38,\n",
       "        38,\n",
       "        4,\n",
       "        3,\n",
       "        3,\n",
       "        4,\n",
       "        3,\n",
       "        3,\n",
       "        4,\n",
       "        3,\n",
       "        3,\n",
       "        3,\n",
       "        104,\n",
       "        104,\n",
       "        104,\n",
       "        104,\n",
       "        104,\n",
       "        104,\n",
       "        104,\n",
       "        104,\n",
       "        104,\n",
       "        103,\n",
       "        105,\n",
       "        104,\n",
       "        105,\n",
       "        104,\n",
       "        104,\n",
       "        105,\n",
       "        104,\n",
       "        105,\n",
       "        104,\n",
       "        104,\n",
       "        5,\n",
       "        4,\n",
       "        5,\n",
       "        4,\n",
       "        5,\n",
       "        4,\n",
       "        5,\n",
       "        4,\n",
       "        5,\n",
       "        4,\n",
       "        9,\n",
       "        9,\n",
       "        9,\n",
       "        8,\n",
       "        9,\n",
       "        9,\n",
       "        8,\n",
       "        9,\n",
       "        9,\n",
       "        8,\n",
       "        41,\n",
       "        40,\n",
       "        40,\n",
       "        41,\n",
       "        40,\n",
       "        40,\n",
       "        41,\n",
       "        40,\n",
       "        40,\n",
       "        40,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        1,\n",
       "        27,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        27,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        26,\n",
       "        103,\n",
       "        103,\n",
       "        103,\n",
       "        103,\n",
       "        103,\n",
       "        103,\n",
       "        103,\n",
       "        103,\n",
       "        103,\n",
       "        102,\n",
       "        32,\n",
       "        32,\n",
       "        32,\n",
       "        32,\n",
       "        32,\n",
       "        32,\n",
       "        32,\n",
       "        32,\n",
       "        32,\n",
       "        31,\n",
       "        39,\n",
       "        38,\n",
       "        39,\n",
       "        38,\n",
       "        39,\n",
       "        38,\n",
       "        39,\n",
       "        38,\n",
       "        39,\n",
       "        38,\n",
       "        117,\n",
       "        116,\n",
       "        116,\n",
       "        116,\n",
       "        116,\n",
       "        117,\n",
       "        116,\n",
       "        116,\n",
       "        116,\n",
       "        116,\n",
       "        ...],\n",
       "       'k': 50}}}}],\n",
       "  'overwrite': False,\n",
       "  'store': False,\n",
       "  'log': False}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_svm.item_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf0a36",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "\n",
    "In F3, all data must be wrapped in the `XYData` class. This ensures that each data transformation is hashed and the results are cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7199ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = XYData(_hash=\" Gambling_2023_train_x\", _path=\"/dataset\", _value=gg_2023_train)\n",
    "train_y = XYData(\n",
    "    _hash=\"Gambling_2023_train_y\", _path=\"/dataset\", _value=gg_2023_train.label.tolist()\n",
    ")\n",
    "\n",
    "test_x = XYData(_hash=\"Gambling_2023_test_x\", _path=\"/dataset\", _value=gg_2023_test)\n",
    "\n",
    "test_y = XYData(\n",
    "    _hash=\"Gambling_2023_test_y\", _path=\"/dataset\", _value=gg_2023_test.label.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badf7849",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157517b1",
   "metadata": {},
   "source": [
    "⚠️ **Warning:** Please note that for parallel backend usage, a considerable amount of RAM will be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "976eeec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GridSearchCV fitting...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">____________________________________________________________________________________________________\n",
       "Fitting pipeline<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "****************************************************************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "____________________________________________________________________________________________________\n",
       "Fitting pipeline\u001b[33m...\u001b[0m\n",
       "****************************************************************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Cached</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">filter</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TWECFilter</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">deltas_f</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'cosine'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'euclidean'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'manhattan'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'chebyshev'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">context_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">cache_data</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">cache_filter</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">overwrite</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">storage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mCached\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mfilter\u001b[0m=\u001b[1;35mTWECFilter\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdeltas_f\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'cosine'\u001b[0m, \u001b[32m'euclidean'\u001b[0m, \u001b[32m'manhattan'\u001b[0m, \u001b[32m'chebyshev'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mcontext_size\u001b[0m=\u001b[1;36m25\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[33mcache_data\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mcache_filter\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33moverwrite\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[33mstorage\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">         - El filtro <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TWECFilter</span><span style=\"font-weight: bold\">({</span><span style=\"color: #008000; text-decoration-color: #008000\">'deltas_f'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'cosine'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'euclidean'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'manhattan'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'chebyshev'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'context_size'</span>: \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span><span style=\"font-weight: bold\">})</span> Existe, se carga del storage.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "         - El filtro \u001b[1;35mTWECFilter\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'deltas_f'\u001b[0m: \u001b[1m[\u001b[0m\u001b[32m'cosine'\u001b[0m, \u001b[32m'euclidean'\u001b[0m, \u001b[32m'manhattan'\u001b[0m, \u001b[32m'chebyshev'\u001b[0m\u001b[1m]\u001b[0m, \u001b[32m'context_size'\u001b[0m: \n",
       "\u001b[1;36m25\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m Existe, se carga del storage.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">         - El dato <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">XYData</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">_hash</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'f991d8f14f3bbdb0a54b565de7e60e42cfd36dc9'</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">_path</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'TWECFilter/60b322a0bd676ce665f0d6b568a28ef664fef914'</span><span style=\"font-weight: bold\">)</span> Existe, se carga del storage.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "         - El dato \u001b[1;35mXYData\u001b[0m\u001b[1m(\u001b[0m\u001b[33m_hash\u001b[0m=\u001b[32m'f991d8f14f3bbdb0a54b565de7e60e42cfd36dc9'\u001b[0m, \n",
       "\u001b[33m_path\u001b[0m=\u001b[32m'TWECFilter/60b322a0bd676ce665f0d6b568a28ef664fef914'\u001b[0m\u001b[1m)\u001b[0m Existe, se carga del storage.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DeltaSelectorFilter</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">deltas_f</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cosine'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDeltaSelectorFilter\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdeltas_f\u001b[0m=\u001b[32m'cosine'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t * Downloading: <_io.BufferedReader name='cache/TWECFilter/60b322a0bd676ce665f0d6b568a28ef664fef914/f991d8f14f3bbdb0a54b565de7e60e42cfd36dc9'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SklearnOptimizer</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">scoring</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'f1_weighted'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">cv</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">pipeline</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">F3Pipeline</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">filters</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ClassifierSVM</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">proba</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metrics</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">F1</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">average</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'weighted'</span><span style=\"font-weight: bold\">)]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">overwrite</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">store</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">log</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">n_jobs</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mSklearnOptimizer\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mscoring\u001b[0m=\u001b[32m'f1_weighted'\u001b[0m,\n",
       "    \u001b[33mcv\u001b[0m=\u001b[1;36m2\u001b[0m,\n",
       "    \u001b[33mpipeline\u001b[0m=\u001b[1;35mF3Pipeline\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mfilters\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mClassifierSVM\u001b[0m\u001b[1m(\u001b[0m\u001b[33mproba\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33mmetrics\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mF1\u001b[0m\u001b[1m(\u001b[0m\u001b[33maverage\u001b[0m=\u001b[32m'weighted'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33moverwrite\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[33mstore\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[33mlog\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mn_jobs\u001b[0m=\u001b[1;36m-1\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "[CV 1/2; 8/9] START ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 2.5}..\n",
      "[CV 2/2; 3/9] START ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 3.0}..\n",
      "[CV 1/2; 3/9] START ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 3.0}..\n",
      "[CV 2/2; 8/9] START ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 2.5}..\n",
      "[CV 1/2; 2/9] START ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 2.5}..\n",
      "[CV 1/2; 6/9] START ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 3.0}..\n",
      "[CV 2/2; 1/9] START ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 1.5}..\n",
      "[CV 1/2; 7/9] START ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 1.5}..\n",
      "[CV 2/2; 4/9] START ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 1.5}..\n",
      "[CV 2/2; 6/9] START ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 3.0}..\n",
      "[CV 1/2; 5/9] START ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 2.5}..\n",
      "[CV 1/2; 9/9] START ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 3.0}..\n",
      "[CV 2/2; 2/9] START ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 2.5}..\n",
      "[CV 2/2; 5/9] START ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 2.5}..\n",
      "[CV 2/2; 7/9] START ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 1.5}..\n",
      "[CV 1/2; 1/9] START ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 1.5}..\n",
      "[CV 2/2; 9/9] START ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 3.0}..\n",
      "[CV 1/2; 4/9] START ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 1.5}..\n",
      "[CV 1/2; 8/9] END ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 2.5};, score=0.965 total time=14.5min\n",
      "[CV 1/2; 5/9] END ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 2.5};, score=0.965 total time=17.1min\n",
      "[CV 1/2; 1/9] END ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 1.5};, score=0.964 total time=18.1min\n",
      "[CV 1/2; 9/9] END ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 3.0};, score=0.965 total time=18.3min\n",
      "[CV 1/2; 6/9] END ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 3.0};, score=0.965 total time=18.5min\n",
      "[CV 2/2; 8/9] END ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 2.5};, score=0.965 total time=18.7min\n",
      "[CV 2/2; 3/9] END ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 3.0};, score=0.965 total time=18.8min\n",
      "[CV 1/2; 2/9] END ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 2.5};, score=0.964 total time=18.9min\n",
      "[CV 2/2; 7/9] END ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 1.5};, score=0.965 total time=19.0min\n",
      "[CV 2/2; 5/9] END ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 2.5};, score=0.966 total time=19.2min\n",
      "[CV 1/2; 7/9] END ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 1.5};, score=0.965 total time=19.9min\n",
      "[CV 2/2; 1/9] END ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 1.5};, score=0.963 total time=19.9min\n",
      "[CV 1/2; 3/9] END ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 3.0};, score=0.964 total time=20.0min\n",
      "[CV 1/2; 4/9] END ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 1.5};, score=0.965 total time=20.0min\n",
      "[CV 2/2; 2/9] END ClassifierSVM__C=1, ClassifierSVM__class_weight_1={1: 2.5};, score=0.964 total time=20.1min\n",
      "[CV 2/2; 4/9] END ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 1.5};, score=0.964 total time=20.5min\n",
      "[CV 2/2; 6/9] END ClassifierSVM__C=3, ClassifierSVM__class_weight_1={1: 3.0};, score=0.966 total time=20.7min\n",
      "[CV 2/2; 9/9] END ClassifierSVM__C=5, ClassifierSVM__class_weight_1={1: 3.0};, score=0.966 total time=20.8min\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">   param_ClassifierSVM__C param_ClassifierSVM__class_weight_1  \\\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>                       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>                            <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.0</span><span style=\"font-weight: bold\">}</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>                       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>                            <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.5</span><span style=\"font-weight: bold\">}</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>                       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>                            <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.0</span><span style=\"font-weight: bold\">}</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>                       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>                            <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.5</span><span style=\"font-weight: bold\">}</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>                       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>                            <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5</span><span style=\"font-weight: bold\">}</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>                       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                            <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.0</span><span style=\"font-weight: bold\">}</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>                       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>                            <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5</span><span style=\"font-weight: bold\">}</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                            <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.5</span><span style=\"font-weight: bold\">}</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>                       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                            <span style=\"font-weight: bold\">{</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5</span><span style=\"font-weight: bold\">}</span>   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>  <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'ClassifierSVM__C'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, 'ClassifierSVM__class_<span style=\"color: #808000; text-decoration-color: #808000\">...</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.965313</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>  <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'ClassifierSVM__C'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, 'ClassifierSVM__class_<span style=\"color: #808000; text-decoration-color: #808000\">...</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.965364</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>  <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'ClassifierSVM__C'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, 'ClassifierSVM__class_<span style=\"color: #808000; text-decoration-color: #808000\">...</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.965114</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>  <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'ClassifierSVM__C'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, 'ClassifierSVM__class_<span style=\"color: #808000; text-decoration-color: #808000\">...</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.965356</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>  <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'ClassifierSVM__C'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, 'ClassifierSVM__class_<span style=\"color: #808000; text-decoration-color: #808000\">...</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.965201</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>  <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'ClassifierSVM__C'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, 'ClassifierSVM__class_<span style=\"color: #808000; text-decoration-color: #808000\">...</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.964449</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'ClassifierSVM__C'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, 'ClassifierSVM__class_<span style=\"color: #808000; text-decoration-color: #808000\">...</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.964589</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'ClassifierSVM__C'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, 'ClassifierSVM__class_<span style=\"color: #808000; text-decoration-color: #808000\">...</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.964195</span>   \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>  <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'ClassifierSVM__C'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, 'ClassifierSVM__class_<span style=\"color: #808000; text-decoration-color: #808000\">...</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.963823</span>   \n",
       "\n",
       "   split1_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.965726</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.965519</span>        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000206</span>                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.965614</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.965489</span>        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000125</span>                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.965626</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.965370</span>        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000256</span>                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.965307</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.965332</span>        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000024</span>                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.965129</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.965165</span>        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000036</span>                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.965158</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.964803</span>        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000354</span>                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.964434</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.964512</span>        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000078</span>                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.964178</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.964186</span>        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000009</span>                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>  \n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.963234</span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.963529</span>        <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.000295</span>                <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span>  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "   param_ClassifierSVM__C param_ClassifierSVM__class_weight_1  \\\n",
       "\u001b[1;36m5\u001b[0m                       \u001b[1;36m3\u001b[0m                            \u001b[1m{\u001b[0m\u001b[1;36m1\u001b[0m: \u001b[1;36m3.0\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m4\u001b[0m                       \u001b[1;36m3\u001b[0m                            \u001b[1m{\u001b[0m\u001b[1;36m1\u001b[0m: \u001b[1;36m2.5\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m8\u001b[0m                       \u001b[1;36m5\u001b[0m                            \u001b[1m{\u001b[0m\u001b[1;36m1\u001b[0m: \u001b[1;36m3.0\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m7\u001b[0m                       \u001b[1;36m5\u001b[0m                            \u001b[1m{\u001b[0m\u001b[1;36m1\u001b[0m: \u001b[1;36m2.5\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m6\u001b[0m                       \u001b[1;36m5\u001b[0m                            \u001b[1m{\u001b[0m\u001b[1;36m1\u001b[0m: \u001b[1;36m1.5\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m2\u001b[0m                       \u001b[1;36m1\u001b[0m                            \u001b[1m{\u001b[0m\u001b[1;36m1\u001b[0m: \u001b[1;36m3.0\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m3\u001b[0m                       \u001b[1;36m3\u001b[0m                            \u001b[1m{\u001b[0m\u001b[1;36m1\u001b[0m: \u001b[1;36m1.5\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m1\u001b[0m                       \u001b[1;36m1\u001b[0m                            \u001b[1m{\u001b[0m\u001b[1;36m1\u001b[0m: \u001b[1;36m2.5\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\u001b[1;36m0\u001b[0m                       \u001b[1;36m1\u001b[0m                            \u001b[1m{\u001b[0m\u001b[1;36m1\u001b[0m: \u001b[1;36m1.5\u001b[0m\u001b[1m}\u001b[0m   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "\u001b[1;36m5\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ClassifierSVM__C'\u001b[0m: \u001b[1;36m3\u001b[0m, 'ClassifierSVM__class_\u001b[33m...\u001b[0m           \u001b[1;36m0.965313\u001b[0m   \n",
       "\u001b[1;36m4\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ClassifierSVM__C'\u001b[0m: \u001b[1;36m3\u001b[0m, 'ClassifierSVM__class_\u001b[33m...\u001b[0m           \u001b[1;36m0.965364\u001b[0m   \n",
       "\u001b[1;36m8\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ClassifierSVM__C'\u001b[0m: \u001b[1;36m5\u001b[0m, 'ClassifierSVM__class_\u001b[33m...\u001b[0m           \u001b[1;36m0.965114\u001b[0m   \n",
       "\u001b[1;36m7\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ClassifierSVM__C'\u001b[0m: \u001b[1;36m5\u001b[0m, 'ClassifierSVM__class_\u001b[33m...\u001b[0m           \u001b[1;36m0.965356\u001b[0m   \n",
       "\u001b[1;36m6\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ClassifierSVM__C'\u001b[0m: \u001b[1;36m5\u001b[0m, 'ClassifierSVM__class_\u001b[33m...\u001b[0m           \u001b[1;36m0.965201\u001b[0m   \n",
       "\u001b[1;36m2\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ClassifierSVM__C'\u001b[0m: \u001b[1;36m1\u001b[0m, 'ClassifierSVM__class_\u001b[33m...\u001b[0m           \u001b[1;36m0.964449\u001b[0m   \n",
       "\u001b[1;36m3\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ClassifierSVM__C'\u001b[0m: \u001b[1;36m3\u001b[0m, 'ClassifierSVM__class_\u001b[33m...\u001b[0m           \u001b[1;36m0.964589\u001b[0m   \n",
       "\u001b[1;36m1\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ClassifierSVM__C'\u001b[0m: \u001b[1;36m1\u001b[0m, 'ClassifierSVM__class_\u001b[33m...\u001b[0m           \u001b[1;36m0.964195\u001b[0m   \n",
       "\u001b[1;36m0\u001b[0m  \u001b[1m{\u001b[0m\u001b[32m'ClassifierSVM__C'\u001b[0m: \u001b[1;36m1\u001b[0m, 'ClassifierSVM__class_\u001b[33m...\u001b[0m           \u001b[1;36m0.963823\u001b[0m   \n",
       "\n",
       "   split1_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "\u001b[1;36m5\u001b[0m           \u001b[1;36m0.965726\u001b[0m         \u001b[1;36m0.965519\u001b[0m        \u001b[1;36m0.000206\u001b[0m                \u001b[1;36m1\u001b[0m  \n",
       "\u001b[1;36m4\u001b[0m           \u001b[1;36m0.965614\u001b[0m         \u001b[1;36m0.965489\u001b[0m        \u001b[1;36m0.000125\u001b[0m                \u001b[1;36m2\u001b[0m  \n",
       "\u001b[1;36m8\u001b[0m           \u001b[1;36m0.965626\u001b[0m         \u001b[1;36m0.965370\u001b[0m        \u001b[1;36m0.000256\u001b[0m                \u001b[1;36m3\u001b[0m  \n",
       "\u001b[1;36m7\u001b[0m           \u001b[1;36m0.965307\u001b[0m         \u001b[1;36m0.965332\u001b[0m        \u001b[1;36m0.000024\u001b[0m                \u001b[1;36m4\u001b[0m  \n",
       "\u001b[1;36m6\u001b[0m           \u001b[1;36m0.965129\u001b[0m         \u001b[1;36m0.965165\u001b[0m        \u001b[1;36m0.000036\u001b[0m                \u001b[1;36m5\u001b[0m  \n",
       "\u001b[1;36m2\u001b[0m           \u001b[1;36m0.965158\u001b[0m         \u001b[1;36m0.964803\u001b[0m        \u001b[1;36m0.000354\u001b[0m                \u001b[1;36m6\u001b[0m  \n",
       "\u001b[1;36m3\u001b[0m           \u001b[1;36m0.964434\u001b[0m         \u001b[1;36m0.964512\u001b[0m        \u001b[1;36m0.000078\u001b[0m                \u001b[1;36m7\u001b[0m  \n",
       "\u001b[1;36m1\u001b[0m           \u001b[1;36m0.964178\u001b[0m         \u001b[1;36m0.964186\u001b[0m        \u001b[1;36m0.000009\u001b[0m                \u001b[1;36m8\u001b[0m  \n",
       "\u001b[1;36m0\u001b[0m           \u001b[1;36m0.963234\u001b[0m         \u001b[1;36m0.963529\u001b[0m        \u001b[1;36m0.000295\u001b[0m                \u001b[1;36m9\u001b[0m  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from joblib import parallel_backend\n",
    "import sys\n",
    "\n",
    "with parallel_backend(\"loky\", n_jobs=-1):\n",
    "    print(\"Starting GridSearchCV fitting...\", flush=True)\n",
    "    pipeline_svm.fit(train_x, train_y)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5c04172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">____________________________________________________________________________________________________\n",
       "Predicting pipeline<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "****************************************************************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "____________________________________________________________________________________________________\n",
       "Predicting pipeline\u001b[33m...\u001b[0m\n",
       "****************************************************************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Cached</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">filter</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TWECFilter</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">deltas_f</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'cosine'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'euclidean'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'manhattan'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'chebyshev'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">context_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">cache_data</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">cache_filter</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">overwrite</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">storage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mCached\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mfilter\u001b[0m=\u001b[1;35mTWECFilter\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdeltas_f\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'cosine'\u001b[0m, \u001b[32m'euclidean'\u001b[0m, \u001b[32m'manhattan'\u001b[0m, \u001b[32m'chebyshev'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mcontext_size\u001b[0m=\u001b[1;36m25\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[33mcache_data\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mcache_filter\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33moverwrite\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
       "    \u001b[33mstorage\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">         - El dato <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">XYData</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">_hash</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'ed20b892e7858a253df46cdd3d19ef040844625d'</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">_path</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'TWECFilter/60b322a0bd676ce665f0d6b568a28ef664fef914'</span><span style=\"font-weight: bold\">)</span> Existe, se carga del storage.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "         - El dato \u001b[1;35mXYData\u001b[0m\u001b[1m(\u001b[0m\u001b[33m_hash\u001b[0m=\u001b[32m'ed20b892e7858a253df46cdd3d19ef040844625d'\u001b[0m, \n",
       "\u001b[33m_path\u001b[0m=\u001b[32m'TWECFilter/60b322a0bd676ce665f0d6b568a28ef664fef914'\u001b[0m\u001b[1m)\u001b[0m Existe, se carga del storage.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DeltaSelectorFilter</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">deltas_f</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'cosine'</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mDeltaSelectorFilter\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdeltas_f\u001b[0m=\u001b[32m'cosine'\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t * Downloading: <_io.BufferedReader name='cache/TWECFilter/60b322a0bd676ce665f0d6b568a28ef664fef914/ed20b892e7858a253df46cdd3d19ef040844625d'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SklearnOptimizer</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">scoring</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'f1_weighted'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">cv</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">pipeline</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">F3Pipeline</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">filters</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ClassifierSVM</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">proba</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">)]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">metrics</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">F1</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">average</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'weighted'</span><span style=\"font-weight: bold\">)]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">overwrite</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">store</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">log</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">n_jobs</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mSklearnOptimizer\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mscoring\u001b[0m=\u001b[32m'f1_weighted'\u001b[0m,\n",
       "    \u001b[33mcv\u001b[0m=\u001b[1;36m2\u001b[0m,\n",
       "    \u001b[33mpipeline\u001b[0m=\u001b[1;35mF3Pipeline\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mfilters\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mClassifierSVM\u001b[0m\u001b[1m(\u001b[0m\u001b[33mproba\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33mmetrics\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mF1\u001b[0m\u001b[1m(\u001b[0m\u001b[33maverage\u001b[0m=\u001b[32m'weighted'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33moverwrite\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[33mstore\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[33mlog\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mn_jobs\u001b[0m=\u001b[1;36m-1\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_y = pipeline_svm.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cac8eb",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7c1808",
   "metadata": {},
   "source": [
    "After training the model on the training set using cross-validation, we evaluate its performance on the test set. This comparison is somewhat biased, as it involves predicting the label of individual chunks while evaluating against labels that were propagated from user-level annotations to their corresponding chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87a89e2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">____________________________________________________________________________________________________\n",
       "Evaluating pipeline<span style=\"color: #808000; text-decoration-color: #808000\">......</span>\n",
       "****************************************************************************************************\n",
       "</pre>\n"
      ],
      "text/plain": [
       "____________________________________________________________________________________________________\n",
       "Evaluating pipeline\u001b[33m...\u001b[0m\u001b[33m...\u001b[0m\n",
       "****************************************************************************************************\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'F1': 0.9881395187504476,\n",
       " 'Precission': 0.9880309912771197,\n",
       " 'Recall': 0.9883182081482912,\n",
       " 'ERDE_5': 3.0647720557294345,\n",
       " 'ERDE_50': 0.959133296979966}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_svm.evaluate(test_x, test_y, _y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c32448f",
   "metadata": {},
   "source": [
    "If we perform a fairer evaluation by propagating the predictions to the user level—assigning a user as positive if at least one of their chunks is predicted as positive—we observe that the performance remains similar or even improves, which indicates that the system is working as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e25a3aac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>chunk</th>\n",
       "      <th>n_texts</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject1</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>[Dope, No retcons or changes. The way it was, ...</td>\n",
       "      <td>[2020-08-03 21:33:23, 2020-08-12 16:41:30, 202...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject1</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>[Where did you get this?, I have no idea how t...</td>\n",
       "      <td>[2020-10-04 22:34:08, 2020-10-04 22:38:55, 202...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject1</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>[A little something im working on, Tried to do...</td>\n",
       "      <td>[2021-02-10 21:13:06, 2021-02-17 20:34:45, 202...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject1</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>[Oh the episodes after the characters stories ...</td>\n",
       "      <td>[2021-04-08 22:55:11, 2021-04-08 23:48:15, 202...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject1</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>[They need to drop easter eggs or hints like t...</td>\n",
       "      <td>[2021-04-25 23:19:00, 2021-04-28 23:10:38, 202...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user  chunk  n_texts  \\\n",
       "0  subject1      0       64   \n",
       "1  subject1      1       64   \n",
       "2  subject1      2       64   \n",
       "3  subject1      3       64   \n",
       "4  subject1      4       64   \n",
       "\n",
       "                                                text  \\\n",
       "0  [Dope, No retcons or changes. The way it was, ...   \n",
       "1  [Where did you get this?, I have no idea how t...   \n",
       "2  [A little something im working on, Tried to do...   \n",
       "3  [Oh the episodes after the characters stories ...   \n",
       "4  [They need to drop easter eggs or hints like t...   \n",
       "\n",
       "                                                date  label  _y  \n",
       "0  [2020-08-03 21:33:23, 2020-08-12 16:41:30, 202...      0   0  \n",
       "1  [2020-10-04 22:34:08, 2020-10-04 22:38:55, 202...      0   0  \n",
       "2  [2021-02-10 21:13:06, 2021-02-17 20:34:45, 202...      0   0  \n",
       "3  [2021-04-08 22:55:11, 2021-04-08 23:48:15, 202...      0   0  \n",
       "4  [2021-04-25 23:19:00, 2021-04-28 23:10:38, 202...      0   0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gg_2023_test[\"_y\"] = _y.value\n",
    "gg_2023_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6aa081af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'F1': 0.9907930136601583,\n",
       " 'Precission': 0.9918003496186526,\n",
       " 'Recall': 0.9903799903799904,\n",
       " 'ERDE_5': 3.3773273408026343,\n",
       " 'ERDE_50': 1.8609453827254776}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux = gg_2023_test.groupby([\"user\"]).agg(\n",
    "    {\"label\": \"first\", \"_y\": lambda x: 1 if any(list(x)) else 0}\n",
    ")\n",
    "{\n",
    "    \"F1\": F1().evaluate(\n",
    "        test_x, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())\n",
    "    ),\n",
    "    \"Precission\": Precission().evaluate(\n",
    "        test_x, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())\n",
    "    ),\n",
    "    \"Recall\": Recall().evaluate(\n",
    "        test_x, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())\n",
    "    ),\n",
    "    \"ERDE_5\": test_erde_5.evaluate(\n",
    "        test_x, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())\n",
    "    ),\n",
    "    \"ERDE_50\": test_erde_50.evaluate(\n",
    "        test_x, XYData.mock(aux.label.tolist()), XYData.mock(aux._y.tolist())\n",
    "    ),\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
