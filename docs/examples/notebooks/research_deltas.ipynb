{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1bfef9f",
   "metadata": {},
   "source": [
    "# Temporal Word Embeddings for Early Detection of Psychological Disorders on Social Media"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc5538f",
   "metadata": {},
   "source": [
    "## How to early detect psychological disorders on social media using temporal word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b372eb",
   "metadata": {},
   "source": [
    "#### Abastract\n",
    "*Mental health disorders represent a public health challenge, where early detection is critical to mitigating adverse outcomes for individuals and society. The study of language and behavior is a pivotal component in mental health research, and the content from social media platforms serves as a valuable tool for identifying signs of mental health risks. This paper presents a novel framework leveraging temporal word embeddings to capture linguistic changes over time. We specifically aim at at identifying emerging psychological concerns on social media. By adapting temporal word representations, our approach quantifies shifts in language use that may signal mental health risks. To that end, we implement two alternative temporal word embedding models to detect linguistic variations and exploit these variations to train early detection classifiers. Our experiments, conducted on 18 datasets from the eRisk initiative (covering signs of conditions such as depression, anorexia, and self-harm), show that simple models focusing exclusively on temporal word usage patterns achieve competitive performance compared to state-of-the-art systems. Additionally, we perform a word-level analysis to understand the evolution of key terms among positive and control users. These findings underscore the potential of time-sensitive word models in this domain, being a promising avenue for future research in mental health surveillance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad778d5",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c40a2a",
   "metadata": {},
   "source": [
    "#### **TWEC**\n",
    "\n",
    "First we difine our temporal word embeddings models. The first model is `TWEC` (Temporal Word Embeddings with A Compass), which extends the Word2Vec model by incorporating temporal information. TWEC captures linguistic changes over time by leveraging the surrounding words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d7c977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import callbacks\n",
    "\n",
    "\n",
    "class MyCallback(callbacks.CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print(\"Pérdida después de la época {}: {}\".format(self.epoch, loss))\n",
    "        else:\n",
    "            print(\n",
    "                \"Pérdida después de la época {}: {}\".format(\n",
    "                    self.epoch, loss - self.loss_previous_step\n",
    "                )\n",
    "            )\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baca2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec, LineSentence, PathLineSentences\n",
    "\n",
    "from gensim import utils\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "import copy\n",
    "from gensim.utils import tokenize\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class TWEC:\n",
    "    \"\"\"\n",
    "    Handles alignment between multiple slices of temporal text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size=100,\n",
    "        sg=0,\n",
    "        siter=10,\n",
    "        ns=10,\n",
    "        window=5,\n",
    "        alpha=0.025,\n",
    "        min_count=5,\n",
    "        workers=2,\n",
    "        test=\"test\",\n",
    "        init_mode=\"hidden\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param size: Number of dimensions. Default is 100.\n",
    "        :param sg: Neural architecture of Word2vec. Default is CBOW (). If 1, Skip-gram is employed.\n",
    "        :param siter: Number of static iterations (epochs). Default is 5.\n",
    "        :param diter: Number of dynamic iterations (epochs). Default is 5.\n",
    "        :param ns: Number of negative sampling examples. Default is 10, min is 1.\n",
    "        :param window: Size of the context window (left and right). Default is 5 (5 left + 5 right).\n",
    "        :param alpha: Initial learning rate. Default is 0.025.\n",
    "        :param min_count: Min frequency for words over the entire corpus. Default is 5.\n",
    "        :param workers: Number of worker threads. Default is 2.\n",
    "        :param test: Folder name of the diachronic corpus files for testing.\n",
    "        :param init_mode: If \\\"hidden\\\" (default), initialize temporal models with hidden embeddings of the context;'\n",
    "                            'if \\\"both\\\", initilize also the word embeddings;'\n",
    "                            'if \\\"copy\\\", temporal models are initiliazed as a copy of the context model\n",
    "                            (same vocabulary)\n",
    "        \"\"\"\n",
    "        self.size = size\n",
    "        self.sg = sg\n",
    "        self.trained_slices = dict()\n",
    "        self.gvocab = []\n",
    "        self.epoch = siter\n",
    "        self.negative = ns\n",
    "        self.window = window\n",
    "        self.static_alpha = alpha\n",
    "        self.dynamic_alpha = alpha\n",
    "        self.min_count = min_count\n",
    "        self.workers = multiprocessing.cpu_count() - 3\n",
    "        self.test = test\n",
    "        self.init_mode = init_mode\n",
    "        self.compass: None | Word2Vec = None\n",
    "\n",
    "    def initialize_from_compass(self, model) -> Word2Vec:\n",
    "        if self.compass is None:\n",
    "            raise Exception(\"Compass model is not initialized\")\n",
    "\n",
    "        if self.init_mode == \"copy\":\n",
    "            model = copy.deepcopy(self.compass)\n",
    "        else:\n",
    "            if self.compass.layer1_size != self.size:  # type: ignore\n",
    "                raise Exception(\"Compass and Slice have different vector sizes\")\n",
    "\n",
    "            if len(model.wv.index_to_key) == 0:\n",
    "                model.build_vocab(corpus_iterable=self.compass.wv.index_to_key)  # type: ignore\n",
    "\n",
    "            vocab_m = model.wv.index_to_key\n",
    "\n",
    "            indices = [\n",
    "                self.compass.wv.key_to_index[w]\n",
    "                for w in vocab_m\n",
    "                if w in self.compass.wv.key_to_index\n",
    "            ]\n",
    "            new_syn1neg = np.array([self.compass.syn1neg[index] for index in indices])\n",
    "            model.syn1neg = new_syn1neg\n",
    "\n",
    "            if self.init_mode == \"both\":\n",
    "                new_syn0 = np.array([self.compass.wv.syn0[index] for index in indices])  # type: ignore\n",
    "                model.wv.syn0 = new_syn0\n",
    "\n",
    "        model.learn_hidden = False  # type: ignore\n",
    "        model.alpha = self.dynamic_alpha\n",
    "        return model\n",
    "\n",
    "    def internal_trimming_rule(self, word, count, min_count):\n",
    "        \"\"\"\n",
    "        Internal rule used to trim words\n",
    "        :param word:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if word in self.gvocab:\n",
    "            return utils.RULE_KEEP\n",
    "        else:\n",
    "            return utils.RULE_DISCARD\n",
    "\n",
    "    def train_model(self, sentences) -> Word2Vec | None:\n",
    "        model = None\n",
    "        if self.compass is None or self.init_mode != \"copy\":\n",
    "            model = Word2Vec(\n",
    "                sg=self.sg,\n",
    "                vector_size=self.size,\n",
    "                alpha=self.static_alpha,\n",
    "                negative=self.negative,\n",
    "                window=self.window,\n",
    "                min_count=self.min_count,\n",
    "                workers=self.workers,\n",
    "            )\n",
    "            model.build_vocab(\n",
    "                corpus_iterable=sentences,\n",
    "                trim_rule=self.internal_trimming_rule\n",
    "                if self.compass is not None\n",
    "                else None,\n",
    "            )\n",
    "\n",
    "        if self.compass is not None:\n",
    "            model = self.initialize_from_compass(model)\n",
    "            model.train(\n",
    "                corpus_iterable=sentences,\n",
    "                total_words=sum([len(s) for s in sentences]),\n",
    "                epochs=self.epoch,\n",
    "                compute_loss=True,\n",
    "            )\n",
    "        else:\n",
    "            model.train(\n",
    "                corpus_iterable=sentences,\n",
    "                total_words=sum([len(s) for s in sentences]),\n",
    "                epochs=self.epoch,\n",
    "                compute_loss=True,\n",
    "                callbacks=[MyCallback()],\n",
    "            )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train_compass(self, chunks):\n",
    "        # sentences = [list(tokenize(frase)) for sentences in chunks for frase in sentences]\n",
    "        sentences = [\n",
    "            list(tokenize(text, lowercase=True, deacc=True)) for text in chunks\n",
    "        ]\n",
    "        print(\"Training the compass.\")\n",
    "        self.compass = self.train_model(sentences)\n",
    "        print(f\"Vocav -> {len(self.compass.wv.index_to_key)}\")\n",
    "        self.gvocab = self.compass.wv.index_to_key\n",
    "\n",
    "    def train_slice(self, chunks):\n",
    "        if self.compass is None:\n",
    "            return Exception(\"Missing Compass\")\n",
    "\n",
    "        # sentences = [list(tokenize(frase)) for sentences in chunks for frase in sentences]\n",
    "        sentences = [list(tokenize(frase)) for frase in chunks]\n",
    "        model = self.train_model(sentences)\n",
    "        return model\n",
    "\n",
    "    # FINE TUNNING VARIATION\n",
    "\n",
    "    def finetune_model(self, sentences, pretrained_path):\n",
    "        model = None\n",
    "        if self.compass is None or self.init_mode != \"copy\":\n",
    "            model = Word2Vec(\n",
    "                sg=self.sg,\n",
    "                vector_size=self.size,\n",
    "                alpha=self.static_alpha,\n",
    "                negative=self.negative,\n",
    "                window=self.window,\n",
    "                min_count=self.min_count,\n",
    "                workers=self.workers,\n",
    "            )\n",
    "            model.build_vocab(\n",
    "                sentences,\n",
    "                trim_rule=self.internal_trimming_rule\n",
    "                if self.compass is not None\n",
    "                else None,\n",
    "            )\n",
    "            # model.build_vocab(list(pretrained_model.vocab.keys()), update=True)\n",
    "            model.intersect_word2vec_format(pretrained_path, binary=True, lockf=1.0)\n",
    "\n",
    "        if self.compass is not None:\n",
    "            model = self.initialize_from_compass(model)\n",
    "\n",
    "        model.train(\n",
    "            sentences,\n",
    "            total_words=sum([len(s) for s in sentences]),\n",
    "            epochs=self.epoch,\n",
    "            compute_loss=True,\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def finetune_compass(self, compass_text, pre_path, overwrite=False, save=True):\n",
    "        sentences = PathLineSentences(compass_text)\n",
    "        sentences.input_files = [\n",
    "            s for s in sentences.input_files if not os.path.basename(s).startswith(\".\")\n",
    "        ]\n",
    "        logging.info(\"Finetunning the compass.\")\n",
    "        self.compass = self.finetune_model(sentences, pre_path)\n",
    "\n",
    "        self.gvocab = self.compass.wv.index_to_key\n",
    "\n",
    "    def finetune_slice(self, slice_text, pretrained):\n",
    "        try:\n",
    "            if self.compass is None:\n",
    "                logging.info(\"Fuck where is the dam compass\")\n",
    "                return Exception(\"Missing Compass\")\n",
    "            logging.info(\n",
    "                \"Finetunning temporal embeddings: slice {}.\".format(slice_text)\n",
    "            )\n",
    "\n",
    "            sentences = LineSentence(slice_text)\n",
    "            model = self.finetune_model(sentences, pretrained)\n",
    "            return model\n",
    "        except Exception as fk:\n",
    "            logging.error(\"What da > {}\".format(fk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef84e3f",
   "metadata": {},
   "source": [
    "#### **DCWE**\n",
    "\n",
    "Then we define the `DCWE` model. This model combines BERT imput embeddings with temporal features the it pass this to BERT model for contextualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a6e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers.models.bert import BertForMaskedLM\n",
    "from torch.nn import functional as F\n",
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "\n",
    "def cosine_similarity(V1, V2):\n",
    "    dot_prod = torch.einsum(\n",
    "        \"abc, cba -> ab\", [V1, V2.permute(*torch.arange(V2.ndim - 1, -1, -1))]\n",
    "    )\n",
    "    norm_1 = torch.norm(V1, dim=-1)\n",
    "    norm_2 = torch.norm(V2, dim=-1)\n",
    "    return dot_prod / torch.einsum(\n",
    "        \"bc, bc -> bc\", norm_1, norm_2\n",
    "    )  # Scores de similitud entre embeddings estáticos y contextualizados\n",
    "\n",
    "\n",
    "def isin(ar1, ar2):\n",
    "    return (ar1[..., None] == ar2).any(-1)\n",
    "\n",
    "\n",
    "class DCWE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        lambda_a,\n",
    "        lambda_w,\n",
    "        vocab_filter=torch.tensor([]),\n",
    "        n_times=10,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super(DCWE, self).__init__()\n",
    "        self.bert = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert_emb_layer = self.bert.get_input_embeddings()\n",
    "        print(f\"Model offset_components = {n_times}\")\n",
    "        self.offset_components = nn.ModuleList(\n",
    "            [OffsetComponent() for _ in range(n_times)]\n",
    "        )\n",
    "        self.lambda_a = lambda_a\n",
    "        self.lambda_w = lambda_w\n",
    "        self.vocab_filter = vocab_filter\n",
    "\n",
    "    # mlm_label, reviews, masks, segs, times, vocab_filter, SA\n",
    "    def forward(self, reviews, times, masks, segs):\n",
    "        bert_embs = self.bert_emb_layer(reviews)\n",
    "\n",
    "        offset_last = torch.cat(\n",
    "            [\n",
    "                self.offset_components[int(j.item())](bert_embs[i])\n",
    "                for i, j in enumerate(F.relu(times.detach().cpu() - 1))\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        offset_now = torch.cat(\n",
    "            [\n",
    "                self.offset_components[int(j.item())](bert_embs[i])\n",
    "                for i, j in enumerate(times.detach().cpu())\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        offset_last = offset_last * (\n",
    "            isin(reviews, self.vocab_filter)\n",
    "        ).float().unsqueeze(-1).expand(-1, -1, 768)\n",
    "        offset_now = offset_now * (isin(reviews, self.vocab_filter)).float().unsqueeze(\n",
    "            -1\n",
    "        ).expand(-1, -1, 768)\n",
    "\n",
    "        input_embs = bert_embs + offset_now\n",
    "\n",
    "        output = self.bert(\n",
    "            inputs_embeds=input_embs,\n",
    "            attention_mask=masks,\n",
    "            token_type_ids=segs,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "\n",
    "        return offset_last, offset_now, output\n",
    "\n",
    "    def loss(self, out, labels, function):\n",
    "        offset_last, offset_now, output = out\n",
    "\n",
    "        logits = output.logits\n",
    "        loss = function(logits.view(-1, self.bert.config.vocab_size), labels.view(-1))\n",
    "        loss += self.lambda_a * torch.norm(offset_now, dim=-1).pow(2).mean()\n",
    "        loss += (\n",
    "            self.lambda_w * torch.norm(offset_now - offset_last, dim=-1).pow(2).mean()\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def generate_deltas(self, texts, input_embs, output_embs, vocab_hash_map, deltas_f):\n",
    "        sim_matrix = deltas_f(input_embs, output_embs).detach().cpu().numpy()\n",
    "\n",
    "        chunk_mat = dok_matrix(\n",
    "            (sim_matrix.shape[0], len(vocab_hash_map)), dtype=np.float32\n",
    "        )\n",
    "        aux_t = texts.cpu().numpy()\n",
    "\n",
    "        for post_idx, post in enumerate(aux_t):\n",
    "            for token in set(post):\n",
    "                if token in vocab_hash_map:\n",
    "                    indices = np.where(post == token)\n",
    "                    # TODO estaba sum ahora mean\n",
    "                    chunk_mat[post_idx, vocab_hash_map[token]] = np.mean(\n",
    "                        sim_matrix[post_idx][indices]\n",
    "                    )\n",
    "\n",
    "        return chunk_mat\n",
    "\n",
    "\n",
    "class OffsetComponent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OffsetComponent, self).__init__()\n",
    "        self.linear_1 = nn.Linear(768, 768)\n",
    "        self.linear_2 = nn.Linear(768, 768)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, embs):\n",
    "        h = self.dropout(torch.tanh(self.linear_1(embs)))\n",
    "        offset = self.linear_2(h).unsqueeze(0)\n",
    "        return offset\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.linear_1 = nn.Linear(768, 100)\n",
    "        self.linear_2 = nn.Linear(100, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, sims):\n",
    "        proj_1 = self.dropout(self.linear_1(sims))\n",
    "        return torch.sigmoid(self.linear_2(proj_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b87c6b",
   "metadata": {},
   "source": [
    "### Deltas\n",
    "\n",
    "Deltas is a measure of movement of word meaning in a text corpus. It's calculated with different similarity measures between the temporal version of the embeddings and the static version of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f153e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def jensen_shannon_divergence(p, q):\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * (\n",
    "        F.kl_div(m.log(), p, reduction=\"none\").sum(-1)\n",
    "        + F.kl_div(m.log(), q, reduction=\"none\").sum(-1)\n",
    "    )\n",
    "\n",
    "\n",
    "def wasserstein_distance(u_values, v_values):\n",
    "    def compute_wasserstein(u, v):\n",
    "        u_sorter = torch.argsort(u, dim=-1)\n",
    "        v_sorter = torch.argsort(v, dim=-1)\n",
    "\n",
    "        all_values = torch.cat([u, v], dim=-1)\n",
    "        all_values, _ = torch.sort(all_values, dim=-1)\n",
    "\n",
    "        deltas = torch.diff(all_values, dim=-1)\n",
    "\n",
    "        u_cdf = torch.searchsorted(\n",
    "            torch.gather(u, -1, u_sorter), all_values[..., :-1], right=True\n",
    "        )\n",
    "        v_cdf = torch.searchsorted(\n",
    "            torch.gather(v, -1, v_sorter), all_values[..., :-1], right=True\n",
    "        )\n",
    "\n",
    "        return torch.sum(torch.abs(u_cdf - v_cdf) * deltas, dim=-1)\n",
    "\n",
    "    distances = torch.stack(\n",
    "        [\n",
    "            compute_wasserstein(u_values[:, i, :], v_values[:, i, :])\n",
    "            for i in range(u_values.shape[1])\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "def pairwise_distance(input_embs, output_embs, p=2):\n",
    "    diff = input_embs - output_embs\n",
    "    distances = torch.norm(diff, p=p, dim=-1)\n",
    "\n",
    "    return distances\n",
    "\n",
    "\n",
    "DISTANCES = {\n",
    "    \"cosine\": lambda input_embs, output_embs: (\n",
    "        1 - F.cosine_similarity(input_embs, output_embs, dim=-1)\n",
    "    )\n",
    "    / 2,\n",
    "    \"euclidean\": lambda input_embs, output_embs: pairwise_distance(\n",
    "        input_embs, output_embs, p=2\n",
    "    ),\n",
    "    \"manhattan\": lambda input_embs, output_embs: pairwise_distance(\n",
    "        input_embs, output_embs, p=1\n",
    "    ),\n",
    "    \"jensen_shannon\": lambda input_embs, output_embs: jensen_shannon_divergence(\n",
    "        input_embs, output_embs\n",
    "    ),\n",
    "    \"wasserstein\": lambda input_embs, output_embs: wasserstein_distance(\n",
    "        input_embs, output_embs\n",
    "    ),\n",
    "    \"chebyshev\": lambda input_embs, output_embs: torch.max(\n",
    "        torch.abs(input_embs - output_embs), dim=-1\n",
    "    ).values,\n",
    "    \"minkowski\": lambda input_embs, output_embs: pairwise_distance(\n",
    "        input_embs, output_embs, p=3\n",
    "    ).pow(1 / 3),\n",
    "}\n",
    "\n",
    "\n",
    "def f_generate_deltas(input_embs, output_embs, distance_metric=\"cosine\"):\n",
    "    if distance_metric in DISTANCES:\n",
    "        sim_matrix = DISTANCES[distance_metric](input_embs, output_embs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distance metric: {distance_metric}\")\n",
    "\n",
    "    return sim_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eac6cd9",
   "metadata": {},
   "source": [
    "## Filters\n",
    "\n",
    "Now that we have defined our models, we need to encapsulated then inside a class that implements BaseFilter interface and binds to the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a48958a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: framework3 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (1.0.10)\n",
      "Requirement already satisfied: boto3==1.35.73 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3) (1.35.73)\n",
      "Requirement already satisfied: cloudpickle==3.1.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3) (3.1.0)\n",
      "Requirement already satisfied: dill==0.3.9 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3) (0.3.9)\n",
      "Requirement already satisfied: fastapi==0.115.5 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3) (0.115.5)\n",
      "Requirement already satisfied: gensim==4.3.3 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3) (4.3.3)\n",
      "Requirement already satisfied: multimethod==1.12 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3) (1.12)\n",
      "Requirement already satisfied: nltk==3.9.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3) (3.9.1)\n",
      "Requirement already satisfied: optuna==4.2.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3) (4.2.1)\n",
      "Requirement already satisfied: pandas==2.2.3 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3) (2.2.3)\n",
      "Requirement already satisfied: pyspark==3.5.3 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3) (3.5.3)\n",
      "Requirement already satisfied: rich==13.9.4 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3) (13.9.4)\n",
      "Requirement already satisfied: scikit-learn==1.5.2 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3) (1.5.2)\n",
      "Requirement already satisfied: scipy==1.13.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3) (1.13.1)\n",
      "Requirement already satisfied: sentence-transformers==4.0.2 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3) (4.0.2)\n",
      "Requirement already satisfied: torch==2.5.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3) (2.5.1)\n",
      "Requirement already satisfied: tqdm==4.67.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3) (4.67.1)\n",
      "Requirement already satisfied: transformers==4.51.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3) (4.51.1)\n",
      "Requirement already satisfied: typeguard==4.4.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3) (4.4.1)\n",
      "Requirement already satisfied: wandb==0.19.9 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from framework3) (0.19.9)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.73 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from boto3==1.35.73->framework3) (1.35.74)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from boto3==1.35.73->framework3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from boto3==1.35.73->framework3) (0.10.4)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from fastapi==0.115.5->framework3) (0.41.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from fastapi==0.115.5->framework3) (2.10.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from fastapi==0.115.5->framework3) (4.12.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from gensim==4.3.3->framework3) (1.26.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from gensim==4.3.3->framework3) (7.0.5)\n",
      "Requirement already satisfied: click in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from nltk==3.9.1->framework3) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from nltk==3.9.1->framework3) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from nltk==3.9.1->framework3) (2024.11.6)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from optuna==4.2.1->framework3) (1.15.2)\n",
      "Requirement already satisfied: colorlog in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from optuna==4.2.1->framework3) (6.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from optuna==4.2.1->framework3) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from optuna==4.2.1->framework3) (2.0.40)\n",
      "Requirement already satisfied: PyYAML in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from optuna==4.2.1->framework3) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from pandas==2.2.3->framework3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from pandas==2.2.3->framework3) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from pandas==2.2.3->framework3) (2024.2)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from pyspark==3.5.3->framework3) (0.10.9.7)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from rich==13.9.4->framework3) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from rich==13.9.4->framework3) (2.18.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from scikit-learn==1.5.2->framework3) (3.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from sentence-transformers==4.0.2->framework3) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from sentence-transformers==4.0.2->framework3) (11.1.0)\n",
      "Requirement already satisfied: filelock in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3) (3.16.1)\n",
      "Requirement already satisfied: networkx in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from torch==2.5.1->framework3) (1.13.1)\n",
      "Requirement already satisfied: requests in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from transformers==4.51.1->framework3) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from transformers==4.51.1->framework3) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from transformers==4.51.1->framework3) (0.4.5)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from wandb==0.19.9->framework3) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from wandb==0.19.9->framework3) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from wandb==0.19.9->framework3) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from wandb==0.19.9->framework3) (5.29.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from wandb==0.19.9->framework3) (6.1.0)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from wandb==0.19.9->framework3) (2.19.0)\n",
      "Requirement already satisfied: setproctitle in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from wandb==0.19.9->framework3) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from wandb==0.19.9->framework3) (75.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.5.1->framework3) (1.3.0)\n",
      "Requirement already satisfied: Mako in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from alembic>=1.5.0->optuna==4.2.1->framework3) (1.3.10)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from botocore<1.36.0,>=1.35.73->boto3==1.35.73->framework3) (2.2.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb==0.19.9->framework3) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb==0.19.9->framework3) (4.0.11)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich==13.9.4->framework3) (0.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.115.5->framework3) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi==0.115.5->framework3) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from requests->transformers==4.51.1->framework3) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from requests->transformers==4.51.1->framework3) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from requests->transformers==4.51.1->framework3) (2024.8.30)\n",
      "Requirement already satisfied: wrapt in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim==4.3.3->framework3) (1.17.0)\n",
      "Requirement already satisfied: greenlet>=1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from sqlalchemy>=1.4.2->optuna==4.2.1->framework3) (3.1.1)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from starlette<0.42.0,>=0.40.0->fastapi==0.115.5->framework3) (4.6.2.post1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from jinja2->torch==2.5.1->framework3) (3.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi==0.115.5->framework3) (1.3.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.19.9->framework3) (5.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install framework3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d7819c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tests.unit'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mframework3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase_clases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseFilter\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mframework3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XYData\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mframework3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Container\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/framework3/venv/lib/python3.11/site-packages/framework3/__init__.py:6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mframework3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplugins\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfilters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mframework3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplugins\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstorage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mframework3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplugins\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mframework3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplugins\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msplitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mframework3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplugins\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/framework3/venv/lib/python3.11/site-packages/framework3/plugins/metrics/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mframework3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplugins\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mframework3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplugins\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mframework3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplugins\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcoherence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/framework3/venv/lib/python3.11/site-packages/framework3/plugins/metrics/classification.py:10\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mframework3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontainer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Container\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mframework3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplugins\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PrecissionKwargs\n\u001b[32m     12\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mF1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPrecission\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRecall\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     15\u001b[39m \u001b[38;5;129m@Container\u001b[39m.bind()\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[34;01mF1\u001b[39;00m(BaseMetric):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/framework3/venv/lib/python3.11/site-packages/framework3/plugins/metrics/utils/types.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Literal, TypedDict\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtests\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01munit\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtest_cached_filter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrayLike\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[34;01mCoherenceEvaluateKwargs\u001b[39;00m(TypedDict, total=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m      7\u001b[39m     f_vocab: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tests.unit'"
     ]
    }
   ],
   "source": [
    "from framework3.base.base_clases import BaseFilter\n",
    "from framework3.base.base_types import XYData\n",
    "from framework3.container import Container\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@Container.bind()\n",
    "class TWECFilter(BaseFilter):\n",
    "    def __init__(self, context_size: int):\n",
    "        self.dCWE = TWEC(size=300, window=context_size)\n",
    "\n",
    "    def fit(self, x: XYData, y: XYData | None) -> float | None:\n",
    "        data: pd.DataFrame = x.value\n",
    "        self.model.train_compass(data.text.values.tolist())\n",
    "        self.vocab_hash_map = dict(\n",
    "            zip(\n",
    "                self.model.compass.wv.index_to_key,\n",
    "                range(len(self.model.compass.wv.index_to_key)),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def predict(self, x: XYData) -> XYData:\n",
    "        data: pd.DataFrame = x.value\n",
    "\n",
    "        all_deltas = {}\n",
    "        for i, row in tqdm(enumerate(data.itertuples()), total=len(data.index)):\n",
    "            tc = self.model.train_slice(row.text)\n",
    "            for word in tc.wv.index_to_key:\n",
    "                if word in self.vocab_hash_map:\n",
    "                    j = self.vocab_hash_map[word]\n",
    "                    # delta_matrix[int(i), j]\n",
    "\n",
    "                    for metric in self.deltas_f:\n",
    "                        aux = all_deltas.get(\n",
    "                            metric,\n",
    "                            dok_matrix(\n",
    "                                (len(data.index), len(self.vocab_hash_map.items())),\n",
    "                                dtype=np.float16,\n",
    "                            ),\n",
    "                        )\n",
    "                        aux[int(i), j] = (\n",
    "                            DISTANCES[metric](\n",
    "                                torch.tensor([[self.model.compass.wv[word]]]),\n",
    "                                torch.tensor([[tc.wv[word]]]),\n",
    "                            )\n",
    "                            .detach()\n",
    "                            .cpu()\n",
    "                            .numpy()\n",
    "                        )\n",
    "                        all_deltas[metric] = aux\n",
    "\n",
    "        return XYData.mock(all_deltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2decb7f",
   "metadata": {},
   "source": [
    "#### DCWE Needs data manipulation classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ba5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "class TimeDatasetNoWindow(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tk, vocab_start=0, vocab_end=10000, **kwargs):\n",
    "        self.vocab_start = vocab_start\n",
    "        self.vocab_end = vocab_end\n",
    "        df.dropna(inplace=True)\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "        self.tok = tk\n",
    "        self.users = df.user.values\n",
    "        self.deltas = df.chunk.values\n",
    "        self.dates = df.date.values\n",
    "        self.labels = df.label.values\n",
    "        self.user2id = {u: i for i, u in enumerate(self.users)}\n",
    "\n",
    "        self.filter_list = [\n",
    "            w\n",
    "            for w in self._top_words(df.text.values.tolist())\n",
    "            if w not in stops and w in self.tok.vocab and w.isalpha()\n",
    "        ][self.vocab_start : self.vocab_end]\n",
    "        self.vocab_filter = torch.tensor(\n",
    "            [t for t in self.tok.encode(self.filter_list) if t >= 2100]\n",
    "        )\n",
    "\n",
    "        df.update(df.text.astype(str).apply(lambda x: x.strip()).apply(self._truncate))\n",
    "        self.texts = df.text.values\n",
    "\n",
    "    def _truncate(self, text):\n",
    "        if len(text) > 512:\n",
    "            text = text[:256] + text[-256:]\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _top_words(self, texts):\n",
    "        vocab = Counter()\n",
    "        for text in texts:\n",
    "            vocab.update(text.strip().split())\n",
    "\n",
    "        total = sum(vocab.values())\n",
    "        vocab = {w: count / total for w, count in vocab.items()}\n",
    "\n",
    "        w_counts = dict()\n",
    "        for w in vocab:\n",
    "            w_counts[w] = w_counts.get(w, 0) + vocab[w]\n",
    "\n",
    "        w_top = sorted(w_counts.keys(), key=lambda x: w_counts[x], reverse=True)\n",
    "\n",
    "        return w_top\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.users[idx]\n",
    "        text = self.texts[idx]\n",
    "        moment = self.deltas[idx]\n",
    "        clazz = self.labels[idx]\n",
    "\n",
    "        return user, text, moment, clazz\n",
    "\n",
    "\n",
    "class TimeCollator:\n",
    "    def __init__(self, user2id, tok, mlm_p=0.15):\n",
    "        self.user2id = user2id\n",
    "        self.tok = tok\n",
    "        self.mlm_p = mlm_p\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        u, t, m, c = zip(*batch)\n",
    "        users = torch.tensor(np.array(list(map(lambda x: self.user2id[x], u))))\n",
    "        moments = torch.tensor(np.array(m)).float()\n",
    "        c = torch.tensor(np.array(c)).int()\n",
    "\n",
    "        # Función que tokeniza y arregla los datos en el formato matricial numpy\n",
    "        def f_app(texts):\n",
    "            input_s = self.tok(\n",
    "                texts.tolist(),\n",
    "                return_tensors=\"pt\",\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "            )\n",
    "\n",
    "            return np.array(\n",
    "                [\n",
    "                    input_s[\"input_ids\"].numpy(),\n",
    "                    input_s[\"token_type_ids\"].numpy(),\n",
    "                    input_s[\"attention_mask\"].numpy(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # target_axis = len(np.array(t).shape) - 1 #Dimensión de trabajo\n",
    "        # Aplica la función auxiliar en la dimensión de trabajo\n",
    "        ans = np.apply_along_axis(f_app, 0, t)\n",
    "        # Desplegamos los datos de la dimensión de trabajo\n",
    "        texts_pad, segs_pad, masks_pad = np.split(ans, ans.shape[0], axis=0)\n",
    "        # Recuperamos el shape (batch, win, emb)\n",
    "        texts_pad = torch.tensor(np.squeeze(texts_pad, axis=0))\n",
    "        segs_pad = torch.tensor(np.squeeze(segs_pad, axis=0))\n",
    "        masks_pad = torch.tensor(np.squeeze(masks_pad, axis=0))\n",
    "\n",
    "        # Ahora tenemos que preparar los datos para MLM\n",
    "        labels = texts_pad.clone()  # Clona los input_ids\n",
    "        #\n",
    "        p_matrix = torch.full(labels.shape, self.mlm_p)  # Matriz de probabilidad\n",
    "        special_mask = np.apply_along_axis(\n",
    "            lambda x: self.tok.get_special_tokens_mask(\n",
    "                x, already_has_special_tokens=True\n",
    "            ),\n",
    "            -1,\n",
    "            labels,\n",
    "        )\n",
    "        p_matrix.masked_fill_(\n",
    "            torch.tensor(special_mask, dtype=torch.bool), value=0.0\n",
    "        )  # Hacemos que se ignoren los tokens especiales\n",
    "        padding_mask = labels.eq(self.tok.pad_token_id)\n",
    "        p_matrix.masked_fill_(\n",
    "            padding_mask, value=0.0\n",
    "        )  # Hacemos que se ignore el token de padding\n",
    "        masked_indices = torch.bernoulli(\n",
    "            p_matrix\n",
    "        ).bool()  # Usamos una distribución de bernoulli\n",
    "        labels[\n",
    "            ~masked_indices\n",
    "        ] = -100  # Esto hace que no se tengan en cuenta los tokens no seleccionados para calcular la pérdida\n",
    "        # Entiendo que hasta aquí se han ignorado una parte de los tokens especiales y de padding siguiendo una distribución de probabilidad\n",
    "\n",
    "        indices_replaced = (\n",
    "            torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        )  # Aplica una lógica de reemplazo de tokens\n",
    "        texts_pad[indices_replaced] = self.tok.convert_tokens_to_ids(\n",
    "            self.tok.mask_token\n",
    "        )\n",
    "\n",
    "        indices_random = (\n",
    "            torch.bernoulli(torch.full(labels.shape, 0.5)).bool()\n",
    "            & masked_indices\n",
    "            & ~indices_replaced\n",
    "        )  # Inserta tokens aleatorios??!?!?!\n",
    "        random_words = torch.randint(len(self.tok), labels.shape, dtype=torch.long)\n",
    "        texts_pad[indices_random] = random_words[indices_random]\n",
    "\n",
    "        return labels, users, moments, c, texts_pad, masks_pad, segs_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1707b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "from pydantic import BaseModel\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.models.bert import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "from rich import print as rprint\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "class ModelConfig(BaseModel):\n",
    "    # clazz: str=Field(...)\n",
    "    lambda_a: float\n",
    "    lambda_w: float\n",
    "    hidden_size: int\n",
    "    # timestamp_kind: str\n",
    "    n_heads: int\n",
    "    drop_prob: float\n",
    "    n_layers: int\n",
    "    time2vec_activation: str\n",
    "\n",
    "\n",
    "class DCWEFilter(BaseFilter):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        lr: float,\n",
    "        n_epoch: int,\n",
    "        batch_size: int,\n",
    "        params: ModelConfig,\n",
    "        deltas_f=[\"cosine\"],\n",
    "        vocab_start: int = 0,\n",
    "        vocab_end: int = 10000,\n",
    "    ):\n",
    "        self.device = torch.device(\n",
    "            \"cuda:{}\".format(0) if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        self.n_epoch = n_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_start = vocab_start\n",
    "        self.vocab_end = vocab_end\n",
    "        if not any([0 if x in DISTANCES else 1 for x in deltas_f]):\n",
    "            self.deltas_f = deltas_f\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Wrong deltas distance function. Available options are: {}.\".format(\n",
    "                    list(DISTANCES.keys())\n",
    "                )\n",
    "            )\n",
    "        self.dcwe = DCWE(**params)  # type: ignore\n",
    "        self.optimizer = AdamW(self.dcwe.parameters(), lr=lr)\n",
    "        self.loss_fn = CrossEntropyLoss()\n",
    "        self.all_models: Dict[float, Any] = dict()\n",
    "        self.vocab_hash_map = None\n",
    "        self.tk = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    def fit(self, x: XYData, y: XYData | None) -> float | None:\n",
    "        print(f\"Vocab {self.vocab_start}:{self.vocab_end}\")\n",
    "        data: pd.DataFrame = x.value\n",
    "        aux_dataset = TimeDatasetNoWindow(\n",
    "            data, tk=self.tk, vocab_start=self.vocab_start, vocab_end=self.vocab_end\n",
    "        )\n",
    "        print(f\"Vocab filter len {len(aux_dataset.vocab_filter)}\")\n",
    "        self.vocab_hash_map = dict(\n",
    "            zip(\n",
    "                aux_dataset.vocab_filter.cpu().tolist(),\n",
    "                range(len(aux_dataset.vocab_filter)),\n",
    "            )\n",
    "        )\n",
    "        self.dcwe.vocab_filter = aux_dataset.vocab_filter.to(self.device)\n",
    "        train_global_count = 0\n",
    "        eval_global_count = 0\n",
    "        train_dataset, valid_dataset = train_test_split(\n",
    "            aux_dataset, train_size=7 / 8, test_size=1 / 8, shuffle=True\n",
    "        )\n",
    "        for epoch in range(1, self.n_epoch + 1):\n",
    "            self.dcwe.train()\n",
    "            losses = list()\n",
    "            start_time = time.time()\n",
    "            collator = TimeCollator(aux_dataset.user2id, aux_dataset.tok)\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=self.batch_size,\n",
    "                collate_fn=collator,\n",
    "                shuffle=True,\n",
    "            )\n",
    "            valid_loader = DataLoader(\n",
    "                valid_dataset,\n",
    "                batch_size=self.batch_size,\n",
    "                collate_fn=collator,\n",
    "                shuffle=True,\n",
    "            )\n",
    "            for batch in tqdm(\n",
    "                train_loader, desc=f\"Runing train batches (epoch {epoch})\"\n",
    "            ):\n",
    "                labels, _, moments, _, texts, masks, segs = batch\n",
    "\n",
    "                labels = labels.to(self.device)\n",
    "                texts = texts.to(self.device)\n",
    "                masks = masks.to(self.device)\n",
    "                segs = segs.to(self.device)\n",
    "                moments = moments.to(self.device)\n",
    "\n",
    "                self.dcwe.zero_grad()\n",
    "\n",
    "                out = self.dcwe(texts, moments, masks, segs)\n",
    "                loss = self.dcwe.loss(out, labels, self.loss_fn)\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                loss.backward()\n",
    "\n",
    "                self.optimizer.step()\n",
    "                train_global_count += 1\n",
    "\n",
    "            perplexity_train = np.exp(np.nanmean(losses))\n",
    "\n",
    "            rprint(\n",
    "                {\n",
    "                    \"perplexity_train\": perplexity_train,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"train_time\": (time.time() - start_time) / 60,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            self.dcwe.eval()\n",
    "            losses = list()\n",
    "            start_time = time.time()\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(\n",
    "                    valid_loader, desc=f\"Evaluating model (epoch {epoch})\"\n",
    "                ):\n",
    "                    labels, _, moments, _, texts, masks, segs = batch\n",
    "\n",
    "                    labels = labels.to(self.device)\n",
    "                    texts = texts.to(self.device)\n",
    "                    masks = masks.to(self.device)\n",
    "                    segs = segs.to(self.device)\n",
    "                    moments = moments.to(self.device)\n",
    "\n",
    "                    out = self.dcwe(texts, moments, masks, segs)\n",
    "                    loss = self.dcwe.loss(out, labels, self.loss_fn)\n",
    "                    losses.append(loss.item())\n",
    "                    eval_global_count += 1\n",
    "\n",
    "            perplexity_eval = np.exp(np.nanmean(losses))\n",
    "            rprint(\n",
    "                {\n",
    "                    \"perplexity_eval\": perplexity_eval,\n",
    "                    \"epoch\": epoch,\n",
    "                    \"eval_time\": (time.time() - start_time) / 60,\n",
    "                }\n",
    "            )\n",
    "            self.all_models[perplexity_eval] = self.dcwe.state_dict()\n",
    "\n",
    "        valid_score, state_dict = max(self.all_models.items(), key=lambda xx: xx[0])\n",
    "        print(f\"Selected model with eval perplexity of {valid_score}\")\n",
    "        self.dcwe.load_state_dict(state_dict)\n",
    "\n",
    "    def predict(self, x: XYData) -> XYData:\n",
    "        all_deltas = {}\n",
    "        data: pd.DataFrame = x.value\n",
    "        x_dataset = TimeDatasetNoWindow(\n",
    "            data, tk=self.tk, vocab_start=self.vocab_start, vocab_end=self.vocab_end\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            collator = TimeCollator(x_dataset.user2id, x_dataset.tok)\n",
    "\n",
    "            train_loader = DataLoader(\n",
    "                x_dataset, batch_size=self.batch_size, collate_fn=collator\n",
    "            )\n",
    "\n",
    "            for i, batch in tqdm(list(enumerate(train_loader)), desc=\"Predicting...\"):\n",
    "                labels, users, moments, clazz, texts, masks, segs = batch\n",
    "\n",
    "                labels = labels.to(self.device)\n",
    "                texts = texts.to(self.device)\n",
    "                masks = masks.to(self.device)\n",
    "                segs = segs.to(self.device)\n",
    "                moments = moments.to(self.device)\n",
    "\n",
    "                _, _, output = self.dcwe(texts, moments, masks, segs)\n",
    "                input_embs = self.dcwe.bert_emb_layer(texts)\n",
    "\n",
    "                for metric in self.deltas_f:\n",
    "                    aux = all_deltas.get(metric, [])\n",
    "                    batch_matrix = self.dcwe.generate_deltas(\n",
    "                        texts,\n",
    "                        input_embs,\n",
    "                        output.hidden_states[-1],\n",
    "                        self.vocab_hash_map,\n",
    "                        DISTANCES[metric],\n",
    "                    )\n",
    "                    aux.append(batch_matrix)\n",
    "                    all_deltas[metric] = aux\n",
    "\n",
    "            for metric in self.deltas_f:\n",
    "                all_deltas[metric] = vstack(all_deltas[metric])\n",
    "\n",
    "        return XYData.mock(all_deltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676eada9",
   "metadata": {},
   "source": [
    "## The classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8730068",
   "metadata": {},
   "source": [
    "This work is an early prediction task of the eRiks, this means we need to use some classifiers. Lets create a set of classifiers and wrap them in a the framework clases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20336758",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68446b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "@Container.bind()\n",
    "class ClassifierSVM(BaseFilter):\n",
    "    def __init__(\n",
    "        self,\n",
    "        C,\n",
    "        kernel,\n",
    "        gamma,\n",
    "        coef0,\n",
    "        tol,\n",
    "        decision_function_shape,\n",
    "        class_weight_1: dict,\n",
    "        probability,\n",
    "    ):\n",
    "        self.proba = probability\n",
    "        self._model = SVC(\n",
    "            C=C,\n",
    "            kernel=kernel,\n",
    "            gamma=gamma,\n",
    "            coef0=coef0,\n",
    "            tol=tol,\n",
    "            decision_function_shape=decision_function_shape,\n",
    "            class_weight={1: class_weight_1},\n",
    "            probability=probability,\n",
    "            random_state=43,\n",
    "        )\n",
    "\n",
    "    def fit(self, x: XYData, y: XYData | None):\n",
    "        if y is None:\n",
    "            raise ValueError(\"y must be provided for training\")\n",
    "        self._model.fit(x.value, y.value)\n",
    "\n",
    "    def predict(self, x: XYData) -> XYData:\n",
    "        if self.proba:\n",
    "            result = list(map(lambda i: i[1], self._model.predict_proba(x.value)))\n",
    "            return XYData.mock(result)\n",
    "        else:\n",
    "            result = self._model.predict(x.value)\n",
    "            return XYData.mock(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9ccb17",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ede0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "class GaussianNaiveBayes(BaseFilter):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators=100,\n",
    "        criterion: Literal[\"gini\", \"entropy\", \"log_loss\"] = \"gini\",\n",
    "        max_depth=2,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        max_features: float | Literal[\"sqrt\", \"log2\"] = \"sqrt\",\n",
    "        class_weight=None,\n",
    "        proba=False,\n",
    "    ):\n",
    "        self.proba = proba\n",
    "        self._model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            criterion=criterion,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            class_weight=class_weight,\n",
    "            random_state=0,\n",
    "        )\n",
    "\n",
    "    def fit(self, x: XYData, y: XYData | None):\n",
    "        if y is None:\n",
    "            raise ValueError(\"y must be provided for training\")\n",
    "        self._model.fit(x.value, y.value)\n",
    "\n",
    "    def predict(self, x: XYData) -> XYData:\n",
    "        if self.proba:\n",
    "            result = list(map(lambda i: i[1], self._model.predict_proba(x.value)))\n",
    "        else:\n",
    "            result = self._model.predict(x.value)\n",
    "        return XYData.mock(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2189c02b",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0670df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.0-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from xgboost) (2.21.5)\n",
      "Requirement already satisfied: scipy in /home/manuel.couto.pintos/Documents/code/framework3/venv/lib/python3.11/site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-3.0.0-py3-none-manylinux_2_28_x86_64.whl (253.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 MB\u001b[0m \u001b[31m102.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f800c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "class XGBClassifierPlugin(BaseFilter):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_estimators,\n",
    "        max_depth,\n",
    "        learning_rate,\n",
    "        min_child_weight,\n",
    "        gamma,\n",
    "        subsample,\n",
    "        colsample_bytree,\n",
    "        reg_alpha,\n",
    "        reg_lambda,\n",
    "        objective=\"binary:logistic\",\n",
    "    ):\n",
    "        self.bst = XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            min_child_weight=min_child_weight,\n",
    "            gamma=gamma,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,  # L2 regularization term on weights, defaults to 1.0\n",
    "            objective=objective,\n",
    "        )\n",
    "\n",
    "    def fit(self, x: XYData, y: XYData | None):\n",
    "        if y is None:\n",
    "            raise ValueError(\"y must be provided for training\")\n",
    "        self.bst.fit(x.value, y.value)\n",
    "\n",
    "    def predict(self, x: XYData) -> XYData:\n",
    "        return self.bst.predict(x.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341ce8a3",
   "metadata": {},
   "source": [
    "## The metrics\n",
    "\n",
    "Also because the early prediction task is a classification task, the metrics we will use will be: F1, Precision and Recall. But because of the early nature of the task we need to use some metrics that penalize the late decisións."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c7a2a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tests.unit'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mframework3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplugins\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F1, Precission, Recall\n\u001b[32m      3\u001b[39m f1 = F1()\n\u001b[32m      4\u001b[39m precision = Precission()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/framework3/venv/lib/python3.11/site-packages/framework3/plugins/metrics/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mframework3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplugins\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mframework3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplugins\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclustering\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mframework3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplugins\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcoherence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/framework3/venv/lib/python3.11/site-packages/framework3/plugins/metrics/classification.py:10\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mframework3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontainer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Container\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mframework3\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplugins\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PrecissionKwargs\n\u001b[32m     12\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mF1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPrecission\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRecall\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     15\u001b[39m \u001b[38;5;129m@Container\u001b[39m.bind()\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[34;01mF1\u001b[39;00m(BaseMetric):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/code/framework3/venv/lib/python3.11/site-packages/framework3/plugins/metrics/utils/types.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Literal, TypedDict\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mtests\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01munit\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtest_cached_filter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrayLike\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[34;01mCoherenceEvaluateKwargs\u001b[39;00m(TypedDict, total=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m      7\u001b[39m     f_vocab: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tests.unit'"
     ]
    }
   ],
   "source": [
    "from framework3.plugins.metrics import F1, Precission, Recall\n",
    "\n",
    "f1 = F1()\n",
    "precision = Precission()\n",
    "recall = Recall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec05a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from framework3 import BaseMetric\n",
    "\n",
    "from numpy import exp\n",
    "\n",
    "\n",
    "@Container.bind()\n",
    "class ERDE(BaseMetric):\n",
    "    def __init__(self, count: Iterable, k: int = 5):\n",
    "        self.k = k\n",
    "        self.count = count\n",
    "\n",
    "    def evaluate(\n",
    "        self, x_data: XYData, y_true: XYData | None, y_pred: XYData\n",
    "    ) -> float | np.ndarray:\n",
    "        if y_true is None:\n",
    "            raise ValueError(\"y_true must be provided for evaluation\")\n",
    "\n",
    "        all_erde = []\n",
    "        _, _, _, tp = confusion_matrix(y_true.value, y_pred.value).ravel()\n",
    "        for expected, result, count in list(\n",
    "            zip(y_true.value, y_pred.value, self.count)\n",
    "        ):\n",
    "            if result == 1 and expected == 0:\n",
    "                all_erde.append(float(tp) / len(y_true.value))\n",
    "            elif result == 0 and expected == 1:\n",
    "                all_erde.append(1.0)\n",
    "            elif result == 1 and expected == 1:\n",
    "                all_erde.append(1.0 - (1.0 / (1.0 + exp(count - self.k))))\n",
    "            elif result == 0 and expected == 0:\n",
    "                all_erde.append(0.0)\n",
    "        return float(np.mean(all_erde) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f3c130",
   "metadata": {},
   "source": [
    "## The pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2818d49e",
   "metadata": {},
   "source": [
    "No it comes the most exiting part, the moment to integrate the filters in the pipeline. This step can be don incrementally wich is most convenient when developping a model but as far as we already know what we are doing we will join all the parts together in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d973f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
