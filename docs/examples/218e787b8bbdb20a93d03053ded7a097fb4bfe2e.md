---
status: new
---
# Tutorial: Investigating Topic Coherence with Framework3 using DWR

In this tutorial, we'll use Framework3 to explore different word embeddings and evaluate topic coherence using Distributed Word Representations (DWR) and the average distance between topic words. We'll create a custom pipeline that incorporates word embeddings, topic modeling, and coherence evaluation.

## Introduction

In the field of natural language processing and topic modeling, evaluating the coherence of topics is a crucial aspect. Distributed Word Representations (DWR) have proven to be a valuable tool for calculating the coherence of representative words in a topic [@nikolenko2016topic]. While Framework3 itself does not incorporate these DWR models, our examples demonstrate how various embedding techniques can be utilized within the framework for topic coherence measurement and coherence metrics evaluation.

It's important to note that there are several approaches to evaluating topic coherence metrics, each with its own strengths and methodologies:

1. Word Intrusion: This method involves inserting a word that doesn't belong into a set of topic words and asking human evaluators to identify the intruder. It provides a direct measure of how well humans can interpret the topic.

2. Large Language Models (LLMs) with Human Oversight: Recent advancements have explored using LLMs to evaluate topic coherence, with human experts validating or refining the LLM's assessments. This approach combines the scalability of automated methods with human judgment.

3. Human Annotations: Direct human evaluation of topic coherence, where experts or crowd workers rate the coherence of topics based on various criteria.

For this investigation, we will be using the strategy employed by [@aletras_evaluating_2013]. We have been generously provided with the human annotations from their work, which offers a valuable ground truth for our coherence metrics evaluations. This approach allows us to directly compare our automated metrics against human judgments, providing a robust benchmark for our coherence measures.


### Word Embeddings

In our examples, we showcase the use of both classic and modern embedding techniques:

- Classic embeddings:
    - Word2Vec [@mikolov2013efficient]
    - GloVe [@pennington2014glove]
    - FastText [@bojanowski2017enriching]

- Modern transformer-based embeddings:
    - BERT [@devlin2018bert]
    - RoBERTa [@liu2019roberta]
    - ALBERT [@lan2019albert]

### Coherence Metrics

Framework3 facilitates the implementation and comparison of various coherence metrics, including both classical and experimental approaches:

- Classical metrics:
    * UMass [@mimno-etal-2011-optimizing]
    * CV (Vector Coherence) [@roder2015exploring]
    * NPMI (Normalized Pointwise Mutual Information) [@bouma2009normalized]
    * UCI [@newman2010automatic]

These metrics offer different perspectives on the quality and coherence of generated topics, potentially enabling a more comprehensive evaluation of topic models. However, it's important to note that the effectiveness of these traditional coherence metrics has been questioned. Somo works criticize the assumption that these metrics are reliable indicators of topic quality or effective in determining the optimal number of topics, especially in domain-specific collections [@doogan_topic_2021]. This critique underscores the need for careful consideration and potentially new approaches when evaluating topic coherence, particularly in specialized domains.

### Methodology

We will follow a methodology similar to [@aletras_evaluating_2013;@nikolenko_topic_2016], with some key differences. Our approach will use various word representations for topic coherence evaluation, and we'll incorporate human evaluations from [@aletras_evaluating_2013] as a gold standard. Here's a detailed breakdown of our methodology:

1. **Word Representations**: We'll use a range of word embedding techniques, including:
   - Classic embeddings: Word2Vec, GloVe, and FastText
   - Modern transformer-based embeddings: BERT, RoBERTa, and ALBERT

2. **Topic Modeling**: We'll use the provided topics from [@aletras_evaluating_2013]. These topics were generated using Latent Dirichlet Allocation (LDA).

3. **Coherence Metrics**: We'll implement and compare various coherence metrics:
   - Classical metrics: UMass, CV, NPMI, and UCI
   - Embedding-based metrics:
     * Average pairwise cosine similarity: This metric calculates the average cosine similarity between all pairs of word embeddings within a topic. It provides a measure of how closely related the words in a topic are in the embedding space.
     * Other potential similarity metrics that could be added in the future include:
       - Euclidean distance: Measures the straight-line distance between word vectors in the embedding space.
       - Manhattan distance: Calculates the sum of the absolute differences of the vector coordinates.
       - Word Mover's Distance (WMD): Measures the minimum distance that the embedded words of one topic need to "travel" to reach the embedded words of another topic.
       - Soft Cosine Similarity: An extension of cosine similarity that takes into account the similarity between different words.
       - Maximum Mean Discrepancy (MMD): A kernel-based method to compare distributions of word embeddings between topics.


4. **Human Evaluations**: We'll use the human evaluations provided by [@aletras_evaluating_2013] as our ground truth for topic coherence.

5. **Correlation Analysis**: To assess the effectiveness of each coherence metric and word representation combination, we'll use Spearman's rank correlation coefficient. This will measure the correlation between the automated coherence scores and the human evaluations.

6. **Comparative Analysis**: We'll compare the performance of different word representations and coherence metrics based on their correlation with human judgments.

By following this methodology, we aim to provide a comprehensive evaluation of topic coherence metrics using various word representations, grounded in human judgments. This approach will allow us to identify which combinations of word embeddings and coherence metrics align most closely with human perceptions of topic quality.

## Implementation Example

In this example, we'll create three classes that extend `BaseFilter` to implement our coherence evaluation pipeline:

1. `GensimEmbedder`: A filter to extract embeddings using Gensim models.
2. `TransformersEmbedder`: A filter to extract embeddings using Transformer models.
3. `PerTopicMeanSimilarity`: A filter to calculate topic coherence based on the average similarity of topic words.

Let's implement these classes:

### 1. GensimEmbedder

```python
from typing import Callable
from framework3 import Container, XYData
from framework3.base import BaseFilter
from tqdm import tqdm

import gensim.downloader as api
import torch
import warnings

warnings.filterwarnings("ignore", message="Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer.")


@Container.bind()
class GensimEmbedder(BaseFilter):
    def __init__(self, model_path: str):
        super().__init__(model_path=model_path)
        self._embeddings:Callable = lambda: api.load(model_path)

    def predict(self, x: XYData) -> XYData:
        self._embeddings = self._embeddings()
        all_m = []
        for topic in tqdm(x.value):
            topic_m = []
            for word in topic:
                try:
                    topic_m.append(self._embeddings[str(word)]) # type: ignore
                except KeyError:
                    topic_m.append([0] * self._embeddings.vector_size) # type: ignore
            all_m.append(torch.tensor(topic_m))

        all_stack = torch.stack(all_m)
        return XYData.mock(all_stack.squeeze(2))
```
### 2. TransformersEmbedder
It is crucial to keep the _model variable private because Transformer classes are not hashable when generating filter hashes. This is essential for the proper functioning of the caching and serialization system.

```python
from transformers import AutoModel, AutoTokenizer
from tqdm import tqdm

from framework3.base import BaseFilter
from framework3 import Container
from framework3.base.base_types import XYData

import torch


@Container.bind()
class TransformersEmbedder(BaseFilter):
    def __init__(self, model_path: str):
        super().__init__(model_path = model_path)
        self.model_path = model_path
        self._tokenizer = lambda: AutoTokenizer.from_pretrained(model_path)
        self._model = lambda: AutoModel.from_pretrained(model_path)


    def predict(self, x: XYData) -> XYData:
        self._tokenizer = self._tokenizer()
        self._model = self._model()

        all_m = []
        for topic in tqdm(x.value):
            topic_m = []
            for word in topic:
                encoded_input = self._tokenizer.encode(
                    str(word),
                    return_tensors='pt'
                )
                out = self._model(input_ids=encoded_input)
                mean_embeddings = torch.mean(
                    out.last_hidden_state[0][1:-1].detach().cpu(),
                    axis=0
                )
                topic_m.append(mean_embeddings)

            all_m.append(torch.stack(topic_m))
        all_stack = torch.stack(all_m)
        return XYData.mock(all_stack.squeeze(2))
```

### 3.PerTopicMeanSimilarity

```python
from torchmetrics.functional.pairwise import pairwise_cosine_similarity

from framework3.base import BaseFilter
from framework3 import Container
from framework3.base.base_types import XYData

import torch

@Container.bind()
class PerTopicMeanSimilarity(BaseFilter):

    def predict(self, x: XYData) -> XYData:
        topic_tensors = x.value

        topic_results = []
        for i in range(topic_tensors.size(0)):
            top_sim = pairwise_cosine_similarity(topic_tensors[i], topic_tensors[i])
            eye = torch.eye(top_sim.shape[0])
            top_sim = top_sim * (1-eye)
            res = torch.sum(top_sim, dim=1) / torch.sum(top_sim>0, dim=1)
            topic_results.append(res)

        return XYData.mock(torch.mean(torch.stack(topic_results), dim=1))

```
### Pipelines Definition

In this section, we'll define our pipelines for evaluating topic coherence using different language models. Before we dive into the pipeline definitions, it's important to note a few key points about our data and approach:

1. **Data Source**: The data we're using for this tutorial has been kindly provided by the authors of Aletras and Stevenson (2013). Due to privacy and licensing considerations, we are unable to make this data publicly available.

2. **Pipeline Structure**: We will create a separate pipeline for each language model we're using. In total, we'll have six pipelines: three for classic word embeddings (Word2Vec, GloVe, and FastText) and three for transformer-based models (BERT, RoBERTa, and ALBERT).

3. **Combining Pipelines**: To efficiently process and compare results from all our models, we'll combine these individual pipelines into a single MonoPipeline.

4. **Visualization**: After running our combined pipeline, we'll visualize the results to compare the coherence scores across different models.

Let's start by defining our individual pipelines:

```python

transformers_models = [
        'bert-base-uncased',
        'roberta-base',
        'albert-base-v2'
    ]

gensim_models = [
    'word2vec-google-news-300',
    'glove-wiki-gigaword-300',
    'fasttext-wiki-news-subwords-300'
]

t_pipelines = [F3Pipeline(
    filters=[
        Cached(
            filter = TransformersEmbedder(model),
            cache_data=True,
            cache_filter=False
        ),
        PerTopicMeanSimilarity()
    ]
) for model in transformers_models]


g_pielines = [F3Pipeline(
    filters=[
        Cached(
            filter = GensimEmbedder(model),
            cache_data=True,
            cache_filter=False
        ),
        PerTopicMeanSimilarity()
    ]
) for model in gensim_models]

pipeline_l = lambda: MonoPipeline(
        filters=[*t_pipelines, *g_pipelines]
    )
```

### Executing Pipelines

Now that we have defined our pipelines, it's time to put them into action. In this section, we'll walk through the process of executing our MonoPipeline across multiple datasets to evaluate topic coherence using various language models.

The execution process involves several key steps:

1. **Data Preparation**: We'll use three datasets, each containing topics that we want to evaluate for coherence. These datasets are represented as XYData objects in our framework.

2. **Pipeline Execution**: Our MonoPipeline, which combines multiple individual pipelines for different language models, will be executed on each dataset.

3. **Result Collection**: For each dataset and model combination, we'll collect the coherence scores produced by our pipeline.

4. **Data Organization**: We'll organize the results into a structured format (a pandas DataFrame) for easy analysis and comparison.

This process will give us a comprehensive view of how different language models perform in assessing topic coherence across various datasets. By executing the pipeline in this systematic way, we can ensure consistency in our evaluation process and facilitate direct comparisons between models and datasets.

In the following code snippet, we'll implement this execution process, demonstrating how to leverage Framework3's capabilities to efficiently process multiple datasets and models in parallel.

```python
final_df = pd.DataFrame()

for name, config in datasets.items():

    #Data preparation
    df = pd.read_csv(config['topics'], header=None, index_col=0)

    X = XYData(
        _hash=name,
        _path='datasets',
        _value=df.loc[:,df.columns[1:]].values
    ) # type: ignore

    # Pipeline instanciation and execution
    pipeline = pipeline_l()

    pipeline.init()

    result = pipeline.predict(X)

    # Results Collection and Organization

    coh_results = pd.read_csv(config['results'], index_col=0)

    groun_truth = XYData.mock(
        torch.tensor(coh_results['humans'].values)
    )

    df = pd.DataFrame(
        preduction2.value,
        columns=transformers_models+gensim_models
    )

    df_combined = pd.concat([coh_results.iloc[:,1:], df], axis=1)

    final_df[name] = df_combined.corr(method='spearman')['humans']


print(final_df.drop(index='humans'))

```
Results are ploted in the command line.
```bash
* Cached({'filter': GensimEmbedder({'model_path': 'word2vec-google-news-300'}), 'cache_data': True, 'cache_filter': False, 'overwrite': False, 'storage': None})
         - El dato XYData(_hash='fb0ce99107cab2c2886814fc2361ed8f95795239', _path='GensimEmbedder/496cbf49bcf06f2e111b0472a88637fc113c157a') Existe, se crea lambda.

* PerTopicMeanSimilarity({})
         * Downloading: <_io.BufferedReader name='/home/manuel.couto.pintos/Documents/code/research_coherence_metrics/cache/GensimEmbedder/496cbf49bcf06f2e111b0472a88637fc113c157a/fb0ce99107cab2c2886814fc2361ed8f95795239'>
____________________________________________________________________________________________________
Predicting pipeline...
****************************************************************************************************
...

* Cached({'filter': GensimEmbedder({'model_path': 'fasttext-wiki-news-subwords-300'}), 'cache_data': True, 'cache_filter': False, 'overwrite': False, 'storage': None})
         - El dato XYData(_hash='218e787b8bbdb20a93d03053ded7a097fb4bfe2e', _path='GensimEmbedder/1e50810d01b16fee88e1f5bef9066480b4db6f24') Existe, se crea lambda.

* PerTopicMeanSimilarity({})
         * Downloading: <_io.BufferedReader name='/home/manuel.couto.pintos/Documents/code/research_coherence_metrics/cache/GensimEmbedder/1e50810d01b16fee88e1f5bef9066480b4db6f24/218e787b8bbdb20a93d03053ded7a097fb4bfe2e'>
                                     20ng       nyt  genomics
c_npmi                           0.475670  0.741326  0.470800
u_mass                           0.545781  0.683995  0.396676
c_v                              0.503730  0.651311  0.472650
c_uci                            0.487970  0.730463  0.449871
cpmi                             0.589414  0.563997  0.061916
bert-base-uncased                0.436957  0.311909  0.205890
roberta-base                     0.309897  0.397525 -0.077470
albert-base-v2                   0.155378  0.304366  0.154627
word2vec-google-news-300         0.581495  0.460657  0.540738
glove-wiki-gigaword-300          0.857683  0.752312  0.796962
fasttext-wiki-news-subwords-300  0.853550  0.521773  0.720706
```

### Analysis of Topic Coherence Metrics

Based on the provided results, we can draw the following conclusions about the correlation between various coherence metrics and human evaluation across different datasets:

1. **Traditional Coherence Metrics**
   - `c_npmi`, `c_uci` and  `c_v` show consistently high correlation across all datasets, suggesting they may be reliable proxies for human judgment of topic coherence.
   - `u_mass` also perform well, particularly for the 'nyt' dataset, indicating their potential effectiveness in news-related contexts.
   - `cpmi` shows variable performance, with a notably low correlation for the 'genomics' dataset, suggesting it may be less reliable for specialized domains.

2. **Word Embedding Models**
   - Word embedding models generally demonstrate stronger correlation with human evaluation compared to transformer-based models:
     - `glove-wiki-gigaword-300` shows the highest overall correlation across all datasets, suggesting it might be the most reliable metric for approximating human judgment.
     - `fasttext-wiki-news-subwords-300` performs exceptionally well, particularly for '20ng' and 'genomics', indicating its strength in handling diverse or specialized vocabularies.
     - `word2vec-google-news-300` shows consistent performance across datasets, though generally lower than GloVe and FastText.

3. **Transformer-Based Models**
   - Transformer models show lower correlation with human evaluation compared to traditional metrics and word embeddings:
     - `bert-base-uncased` performs the best among transformers, but still falls short of word embedding models.
     - `roberta-base` and `albert-base-v2` show particularly low correlation, with `roberta-base` even showing a negative correlation for the 'genomics' dataset.

4. **Dataset-Specific Observations**
   - The 'nyt' dataset shows the highest correlations across most metrics, suggesting that news articles may produce more coherent topics that align well with human judgment.
   - The 'genomics' dataset presents challenges for some metrics, particularly `cpmi` and transformer models, highlighting the importance of choosing appropriate metrics for specialized domains.

### Implications for Topic Modeling Evaluation

1. Traditional coherence metrics, particularly `c_npmi`, `c_uci` and `c_v`, remain robust choices for evaluating topic coherence across different domains.
2. Word embedding models, especially GloVe and FastText, show the strongest correlation with human evaluation and should be considered as primary metrics for assessing topic coherence.
3. Transformer-based models, despite their success in other NLP tasks, may not be the best choice for evaluating topic coherence, at least in their current implementation.
4. When evaluating topic models, it's advisable to use a combination of traditional metrics (e.g., `c_npmi`) and word embedding-based metrics (e.g., GloVe or FastText) for a more comprehensive assessment that likely aligns better with human judgment.

## Framework3 for Topic Modeling and Coherence Metrics Evaluation

Framework3 offers a comprehensive solution for researchers and practitioners working with topic models and coherence metrics. Its unique features make it particularly well-suited for experimenting with and evaluating different approaches to topic modeling and coherence assessment.

### Key Features for Topic Modeling Research

1. **Unified Metric Integration**: Seamlessly incorporate diverse coherence metrics, from traditional measures like NPMI and UCI to advanced embedding-based techniques, allowing for comprehensive comparisons.

2. **Flexible Topic Model Integration**: Easily experiment with various topic modeling algorithms, including LDA, NMF, and more recent neural approaches, within a single framework.

3. **Customizable Evaluation Pipelines**: Design and execute complex evaluation workflows that combine multiple metrics, models, and datasets, facilitating thorough comparative analyses.

4. **Extensible Plugin Architecture**: Rapidly implement and test new coherence metrics or topic modeling techniques through a plugin system, promoting innovation and experimentation.

5. **Scalable Processing**: Leverage distributed computing capabilities to handle large-scale topic modeling tasks and coherence evaluations across extensive corpora.

## References
\bibliography
